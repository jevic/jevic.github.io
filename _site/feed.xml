<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://0.0.0.0/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0/" rel="alternate" type="text/html" /><updated>2018-11-27T16:42:21+08:00</updated><id>http://0.0.0.0/</id><title type="html">Jevic</title><subtitle>......</subtitle><author><name>Jevic</name></author><entry><title type="html">elastic-stack 6.5</title><link href="http://0.0.0.0/2018/11/22/elastic-stack-6.5/" rel="alternate" type="text/html" title="elastic-stack 6.5" /><published>2018-11-22T19:56:06+08:00</published><updated>2018-11-22T19:56:06+08:00</updated><id>http://0.0.0.0/2018/11/22/elastic-stack-6.5</id><content type="html" xml:base="http://0.0.0.0/2018/11/22/elastic-stack-6.5/">&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;在以往的旧版本(2.x,5.x) 每个索引可以存储不同类型的文档,
      &lt;ul&gt;
        &lt;li&gt;类比MySQL
          &lt;ul&gt;
            &lt;li&gt;index == database&lt;/li&gt;
            &lt;li&gt;_type == table&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;6.x 版本开始 移除了_type  也就是每个索引只有一种类型！！！&lt;/li&gt;
    &lt;li&gt;x-pack 从6.3版本开始已经内置在elasticsearch,kibana 当中无需另行安装!&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;elasticsearch&quot;&gt;elasticsearch&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.elastic.co/downloads&quot;&gt;官网下载&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;下载解压&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;配置基础环境&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# cat /etc/security/limit.conf
* soft nofile 65536
* hard nofile 65536
* soft nproc unlimited
* hard nproc unlimited
es soft memlock unlimited
es hard memlock unlimited
  
# cat /etc/sysctl.conf
vm.swappiness = 1
vm.max_map_count=262144
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;添加 es 用户并授权&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;启动&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;授权 license （30天试用版）&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;证书申请: https://register.elastic.co/marvel_register&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -H &quot;Content-Type:application/json&quot; -XPOST  http://192.168.2.221:9200/_xpack/license/start_trial?acknowledge=true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;x-pack-开启认证&quot;&gt;x-pack 开启认证&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;配置用户名密码:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$ES_PATH:/bin/elasticsearch-setup-passwords interactive&lt;/li&gt;
      &lt;li&gt;根据提示一步步设置密码即可&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;修改 elasticsearch 配置文件&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# tail elasticsearch.yml -n 1
xpack.security.enabled: true  ## 开启认证
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;重启ES，再次访问则需要输入用户名密码&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;修改密码&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -H &quot;Content-Type:application/json&quot; -XPOST -u elastic 'http://192.168.2.221:9200/_xpack/security/user/elastic/_password' -d '{ &quot;password&quot; : &quot;123456&quot; }'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;kibana&quot;&gt;kibana&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;配置对应的用户名密码以及 ES 链接地址&lt;/li&gt;
  &lt;li&gt;配置文件添加此配置:
    &lt;ul&gt;
      &lt;li&gt;xpack.security.enabled: true&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;启动即可&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cerebro&quot;&gt;cerebro&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/lmenezes/cerebro&quot;&gt;github&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;具体步骤查看文档即可&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sql-插件&quot;&gt;sql 插件&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/NLPchina/elasticsearch-sql&quot;&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;转载请注明出处，本文采用 &lt;a href=&quot;http://creativecommons.org/licenses/by-nc-nd/4.0/&quot;&gt;CC4.0&lt;/a&gt; 协议授权&lt;/p&gt;</content><author><name>Jevic</name></author><summary type="html">在以往的旧版本(2.x,5.x) 每个索引可以存储不同类型的文档, 类比MySQL index == database _type == table 6.x 版本开始 移除了_type 也就是每个索引只有一种类型！！！ x-pack 从6.3版本开始已经内置在elasticsearch,kibana 当中无需另行安装!</summary></entry><entry><title type="html">二进制手动部署kubernetes 1.10.10</title><link href="http://0.0.0.0/2018/09/23/kuberentes-1.10.10/" rel="alternate" type="text/html" title="二进制手动部署kubernetes 1.10.10" /><published>2018-09-23T20:35:46+08:00</published><updated>2018-09-23T20:35:46+08:00</updated><id>http://0.0.0.0/2018/09/23/kuberentes-1.10.10</id><content type="html" xml:base="http://0.0.0.0/2018/09/23/kuberentes-1.10.10/">&lt;blockquote&gt;
  &lt;p&gt;通读一遍在实际操作!!!
关于镜像请查看最后的补充说明
部署脚本查看&lt;a href=&quot;https://github.com/jevic/kshell.git&quot;&gt;仓库脚本&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;一-系统环境&quot;&gt;一. 系统环境&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;CentOS Linux release 7.2.1511 (Core)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;IP地址&lt;/th&gt;
      &lt;th&gt;主机名&lt;/th&gt;
      &lt;th&gt;Docker 版本&lt;/th&gt;
      &lt;th&gt;kubernetes 版本&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;192.168.2.219&lt;/td&gt;
      &lt;td&gt;master219&lt;/td&gt;
      &lt;td&gt;18.06.0-ce&lt;/td&gt;
      &lt;td&gt;v1.10.10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;192.168.2.220&lt;/td&gt;
      &lt;td&gt;node220&lt;/td&gt;
      &lt;td&gt;18.06.0-ce&lt;/td&gt;
      &lt;td&gt;v1.10.10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;192.168.2.221&lt;/td&gt;
      &lt;td&gt;node221&lt;/td&gt;
      &lt;td&gt;18.06.0-ce&lt;/td&gt;
      &lt;td&gt;v1.10.10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;初始化&quot;&gt;初始化&lt;/h3&gt;
&lt;h4 id=&quot;11-配置ssh&quot;&gt;1.1 配置ssh&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh-keygen -t rsa
一路回车....

将公钥添加到每个节点
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;批量执行脚本:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@master219 bin]# pwd
/usr/local/bin
[root@master219 bin]# cat cmd
#!/bin/bash
## 批量执行命令
iplist=&quot;node220 node221&quot;

for i in $iplist
do
   echo -e &quot;\033[32mssh $i \&quot;$*\&quot;\033[0m&quot;
   ssh $i &quot;$*&quot;
done
[root@master219 bin]# cat rsy
#!/bin/bash
## 批量同步文件
iplist=&quot;node220 node221&quot;

for i in $iplist
do
   echo -e &quot;\033[32mscp -r $1 $i:$2\033[0m&quot;
   scp -r $1 $i:$2
done

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;12-系统配置&quot;&gt;1.2 系统配置&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;所有节点都需要执行下列操作&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=disalbe/g' /etc/sysconfig/selinux

cat &amp;gt;&amp;gt; /etc/security/limits.conf &amp;lt;&amp;lt;EOF
* soft nofile 65536
* hard nofile 65536
* soft nproc unlimited
* hard nproc unlimited
EOF

swapoff -a

cat &amp;gt;&amp;gt; /etc/sysctl.conf &amp;lt;&amp;lt;EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
net.ipv4.tcp_tw_recycle=0
vm.swappiness=0
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.ipv6.conf.all.disable_ipv6=1
net.netfilter.nf_conntrack_max=2310720
EOF

systemctl stop firewalld &amp;amp;&amp;amp; systemctl disable firewalld
yum makecache fast
yum install -y epel-release
yum install -y conntrack ipvsadm ipset jq sysstat curl iptables libseccomp ntpdate
ntpdate cn.pool.ntp.org
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;13-install-docker&quot;&gt;1.3 Install Docker&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;参考 &lt;a href=&quot;https://docs.docker.com/install/linux/docker-ce/centos/#install-using-the-repository&quot;&gt;官方文档&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yum install -y yum-utils device-mapper-persistent-data lvm2
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum list docker-ce --showduplicates | sort -r
yum -y install docker-ce-18.06.0.ce
systemctl start docker &amp;amp;&amp;amp; systemctl stop docker
cat &amp;gt; /etc/docker/daemon.json &amp;lt;&amp;lt;EOF
{
  &quot;registry-mirrors&quot;: [&quot;https://dlvqhrac.mirror.aliyuncs.com&quot;]
}
EOF

systemctl daemon-reload
systemctl start docker
systemctl enable docker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;二-安装cfssl&quot;&gt;二. 安装cfssl&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://github.com/cloudflare/cfssl/releases&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;只需要在master节点安装使用即可，后续证书同步到其他节点即可&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tar -zxvf cfssl.tar.gz
mv cfssl cfssljson /usr/local/bin
chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson
rm -f cfssl.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;三-etcd&quot;&gt;三. ETCD&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/coreos/etcd/releases/download&quot;&gt;下载链接&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;创建etcd 用户来启动!! 注意用户权限&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;31-证书&quot;&gt;3.1 证书&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;etcd-csr.json&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;O&quot;: &quot;etcd&quot;,
      &quot;OU&quot;: &quot;etcd Security&quot;,
      &quot;L&quot;: &quot;Shengzhen&quot;,
      &quot;ST&quot;: &quot;Shengzhen&quot;,
      &quot;C&quot;: &quot;CN&quot;
    }
  ],
  &quot;CN&quot;: &quot;etcd&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;localhost&quot;,
    &quot;192.168.2.219&quot;,
    &quot;192.168.2.220&quot;,
    &quot;192.168.2.221&quot;
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;etcd-gencert.json&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;signing&quot;: {
    &quot;default&quot;: {
        &quot;usages&quot;: [
          &quot;signing&quot;,
          &quot;key encipherment&quot;,
          &quot;server auth&quot;,
          &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;87600h&quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;etcd-root-ca-csr.json&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 4096
  },
  &quot;names&quot;: [
    {
      &quot;O&quot;: &quot;etcd&quot;,
      &quot;OU&quot;: &quot;etcd Security&quot;,
      &quot;L&quot;: &quot;Shengzhen&quot;,
      &quot;ST&quot;: &quot;Shengzhen&quot;,
      &quot;C&quot;: &quot;CN&quot;
    }
  ],
  &quot;CN&quot;: &quot;etcd-root-ca&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;生成证书&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cfssl gencert --initca=true etcd-root-ca-csr.json | cfssljson --bare etcd-root-ca
cfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd

[root@master219 ssl]# ls
etcd.csr  etcd-csr.json  etcd-gencert.json  etcd-key.pem  etcd.pem  etcd-root-ca.csr  etcd-root-ca-csr.json  etcd-root-ca-key.pem  etcd-root-ca.pem
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;32-etcdconf&quot;&gt;3.2 etcd.conf&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# [member]
ETCD_NAME=etcd1
ETCD_DATA_DIR=&quot;/var/lib/etcd/etcd1.etcd&quot;
ETCD_WAL_DIR=&quot;/var/lib/etcd/wal&quot;
ETCD_SNAPSHOT_COUNT=&quot;100&quot;
ETCD_HEARTBEAT_INTERVAL=&quot;100&quot;
ETCD_ELECTION_TIMEOUT=&quot;1000&quot;
ETCD_LISTEN_PEER_URLS=&quot;https://192.168.2.219:2380&quot;
ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.2.219:2379,http://127.0.0.1:2379&quot;
ETCD_MAX_SNAPSHOTS=&quot;5&quot;
ETCD_MAX_WALS=&quot;5&quot;
#ETCD_CORS=&quot;&quot;
# [cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.2.219:2380&quot;
ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.2.219:2379&quot;
# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;
ETCD_INITIAL_CLUSTER=&quot;etcd1=https://192.168.2.219:2380,etcd2=https://192.168.2.220:2380,etcd3=https://192.168.2.221:2380&quot;
ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;
ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;
#ETCD_DISCOVERY=&quot;&quot;
#ETCD_DISCOVERY_SRV=&quot;&quot;
#ETCD_DISCOVERY_FALLBACK=&quot;proxy&quot;
#ETCD_DISCOVERY_PROXY=&quot;&quot;
#ETCD_STRICT_RECONFIG_CHECK=&quot;false&quot;
#ETCD_AUTO_COMPACTION_RETENTION=&quot;0&quot;
# [proxy]
#ETCD_PROXY=&quot;off&quot;
#ETCD_PROXY_FAILURE_WAIT=&quot;5000&quot;
#ETCD_PROXY_REFRESH_INTERVAL=&quot;30000&quot;
#ETCD_PROXY_DIAL_TIMEOUT=&quot;1000&quot;
#ETCD_PROXY_WRITE_TIMEOUT=&quot;5000&quot;
#ETCD_PROXY_READ_TIMEOUT=&quot;0&quot;
# [security]
ETCD_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;
ETCD_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;
ETCD_CLIENT_CERT_AUTH=&quot;true&quot;
ETCD_TRUSTED_CA_FILE=&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;
ETCD_AUTO_TLS=&quot;true&quot;
ETCD_PEER_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;
ETCD_PEER_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;
ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;
ETCD_PEER_TRUSTED_CA_FILE=&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;
ETCD_PEER_AUTO_TLS=&quot;true&quot;
# [logging]
#ETCD_DEBUG=&quot;false&quot;
# examples for -log-package-levels etcdserver=WARNING,security=DEBUG
#ETCD_LOG_PACKAGE_LEVELS=&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;33-etcdservice&quot;&gt;3.3 etcd.service&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
# set GOMAXPROCS to number of processors
ExecStart=/bin/bash -c &quot;GOMAXPROCS=$(nproc) /usr/local/bin/etcd --name=\&quot;${ETCD_NAME}\&quot; --data-dir=\&quot;${ETCD_DATA_DIR}\&quot; --listen-client-urls=\&quot;${ETCD_LISTEN_CLIENT_URLS}\&quot;&quot;
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;34-安装启动&quot;&gt;3.4 安装启动&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;install.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ETCD_VERSION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;3.2.18&quot;&lt;/span&gt;

preinstall&lt;span class=&quot;o&quot;&gt;(){&lt;/span&gt;
    getent group etcd &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;/dev/null &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; groupadd &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; etcd
    getent passwd etcd &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;/dev/null &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; useradd &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-g&lt;/span&gt; etcd &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; /var/lib/etcd &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; /sbin/nologin &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;etcd user&quot;&lt;/span&gt; etcd
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

install&lt;span class=&quot;o&quot;&gt;(){&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\0&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;33[32mINFO: Copy etcd...&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\0&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;33[0m&quot;&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;tar&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-zxvf&lt;/span&gt; etcd-v&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ETCD_VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-linux-amd64&lt;/span&gt;.tar.gz
    cp etcd-v&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ETCD_VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-linux-amd64&lt;/span&gt;/etcd&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; /usr/local/bin
    rm &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; etcd-v&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ETCD_VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-linux-amd64&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\0&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;33[32mINFO: Copy etcd config...&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\0&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;33[0m&quot;&lt;/span&gt;
    cp &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; conf /etc/etcd
    chown &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; etcd:etcd /etc/etcd
    chmod &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; 755 /etc/etcd/ssl
    &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\0&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;33[32mINFO: Copy etcd systemd config...&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\0&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;33[0m&quot;&lt;/span&gt;
    cp etcd.service /lib/systemd/system
    systemctl daemon-reload
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

postinstall&lt;span class=&quot;o&quot;&gt;(){&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;/var/lib/etcd&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then
        &lt;/span&gt;mkdir /var/lib/etcd
        chown &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; etcd:etcd /var/lib/etcd
    &lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

preinstall
install
postinstall

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;目录结构:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;etcd/
├── conf
│   ├── etcd.conf
│   └── ssl
│       ├── etcd.csr
│       ├── etcd-csr.json
│       ├── etcd-gencert.json
│       ├── etcd-key.pem
│       ├── etcd.pem
│       ├── etcd-root-ca.csr
│       ├── etcd-root-ca-csr.json
│       ├── etcd-root-ca-key.pem
│       └── etcd-root-ca.pem
├── etcd.service
├── etcd-v3.2.18-linux-amd64.tar.gz
└── install.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建对应目录以及相关文件，保存 install.sh 脚本;将整个目录同步到各个节点执行即可；
然后在各个节点修改对应的etcd 名称及IP即可；&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl start etcd  ## 三个节点同时执行此操作否则单个启动会卡死在那里
[root@master219 mnt]# cmd systemctl enable  ## 使用前面配置的cmd 命令批量执行
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export ETCDCTL_API=3
etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.2.219:2379,https://192.168.2.220:2379,https://192.168.2.221:2379 endpoint health

https://192.168.2.221:2379 is healthy: successfully committed proposal: took = 2.077429ms
https://192.168.2.219:2379 is healthy: successfully committed proposal: took = 1.421477ms
https://192.168.2.220:2379 is healthy: successfully committed proposal: took = 2.222464ms
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;四-kubernetes&quot;&gt;四. kubernetes&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;基于 hyperkube 二进制手动安装&lt;/li&gt;
  &lt;li&gt;hyperkube是一个集成二进制运行文件&lt;/li&gt;
  &lt;li&gt;可以使用“hyperkube kubelet …”来启动kubelet ，用“hyperkube apiserver …”运行apiserver 等等。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;41-证书&quot;&gt;4.1 证书&lt;/h3&gt;
&lt;p&gt;由于 kubelet 和 kube-proxy 用到的 kubeconfig 配置文件需要借助 kubectl 来生成，所以需要先安装一下 kubectl&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl https://storage.googleapis.com/kubernetes-release/release/v1.10.10/bin/linux/amd64/hyperkube -o hyperkube-1.10.10
chmod +x hyperkube_1.10.10
cp hyperkube_1.10.10 /usr/local/bin/hyperkube
ln -s /usr/local/bin/hyperkube /usr/local/bin/kubectl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://www.jevic.cn/2018/03/19/shadowsock/&quot;&gt;curl 代理配置&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;admin-csrjson&quot;&gt;admin-csr.json&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;CN&quot;: &quot;admin&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;Shengzhen&quot;,
      &quot;L&quot;: &quot;Shengzhen&quot;,
      &quot;O&quot;: &quot;system:masters&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;k8s-gencertjson&quot;&gt;k8s-gencert.json&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;87600h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;87600h&quot;
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;k8s-root-ca-csrjson&quot;&gt;k8s-root-ca-csr.json&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 4096
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;Shengzhen&quot;,
      &quot;L&quot;: &quot;Shengzhen&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;kube-apiserver-csrjson&quot;&gt;kube-apiserver-csr.json&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;CN&quot;: &quot;kubernetes&quot;,
    &quot;hosts&quot;: [
        &quot;127.0.0.1&quot;,
        &quot;10.254.0.1&quot;,
        &quot;192.168.2.219&quot;,
        &quot;192.168.2.220&quot;,
        &quot;192.168.2.221&quot;,
        &quot;*.kubernetes.master&quot;,
        &quot;localhost&quot;,
        &quot;kubernetes&quot;,
        &quot;kubernetes.default&quot;,
        &quot;kubernetes.default.svc&quot;,
        &quot;kubernetes.default.svc.cluster&quot;,
        &quot;kubernetes.default.svc.cluster.local&quot;
    ],
    &quot;key&quot;: {
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    },
    &quot;names&quot;: [
        {
            &quot;C&quot;: &quot;CN&quot;,
            &quot;ST&quot;: &quot;Shengzhen&quot;,
            &quot;L&quot;: &quot;Shengzhen&quot;,
            &quot;O&quot;: &quot;k8s&quot;,
            &quot;OU&quot;: &quot;System&quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;kube-proxy-csrjson&quot;&gt;kube-proxy-csr.json&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;CN&quot;: &quot;system:kube-proxy&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;Shengzhen&quot;,
      &quot;L&quot;: &quot;Shengzhen&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;411-生成证书和配置&quot;&gt;4.1.1 生成证书和配置&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 生成 CA
cfssl gencert --initca=true k8s-root-ca-csr.json | cfssljson --bare k8s-root-ca

# 依次生成其他组件证书
for targetName in kube-apiserver admin kube-proxy; do
    cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes $targetName-csr.json | cfssljson --bare $targetName
done

# 地址默认为 127.0.0.1:6443
# 如果在 master 上启用 kubelet 请在生成后的 kubeconfig 中
# 修改该地址为 当前MASTER_IP:6443
KUBE_APISERVER=&quot;https://127.0.0.1:6443&quot;
BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')
echo &quot;Tokne: ${BOOTSTRAP_TOKEN}&quot;

# 不要质疑 system:bootstrappers 用户组是否写错了，有疑问请参考官方文档
# https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/
cat &amp;gt; token.csv &amp;lt;&amp;lt;EOF
${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&quot;system:bootstrappers&quot;
EOF

echo &quot;Create kubelet bootstrapping kubeconfig...&quot;
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=k8s-root-ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=bootstrap.kubeconfig
# 设置客户端认证参数
kubectl config set-credentials kubelet-bootstrap \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=bootstrap.kubeconfig
# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig
# 设置默认上下文
kubectl config use-context default --kubeconfig=bootstrap.kubeconfig

echo &quot;Create kube-proxy kubeconfig...&quot;
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=k8s-root-ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig
# 设置客户端认证参数
kubectl config set-credentials kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig
# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig
# 设置默认上下文
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig

# 创建高级审计配置
cat &amp;gt;&amp;gt; audit-policy.yaml &amp;lt;&amp;lt;EOF
# Log all requests at the Metadata level.
apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
- level: Metadata
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;生成后的文件&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssl
├── admin.csr
├── admin-csr.json
├── admin-key.pem
├── admin.pem
├── audit-policy.yaml
├── bootstrap.kubeconfig
├── genconfig.sh
├── k8s-gencert.json
├── k8s-root-ca.csr
├── k8s-root-ca-csr.json
├── k8s-root-ca-key.pem
├── k8s-root-ca.pem
├── kube-apiserver.csr
├── kube-apiserver-csr.json
├── kube-apiserver-key.pem
├── kube-apiserver.pem
├── kube-proxy.csr
├── kube-proxy-csr.json
├── kube-proxy-key.pem
├── kube-proxy.kubeconfig
├── kube-proxy.pem
└── token.csv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;42-systemd-配置&quot;&gt;4.2 systemd 配置&lt;/h3&gt;

&lt;h4 id=&quot;kube-apiserverservice&quot;&gt;kube-apiserver.service&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service
[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
User=root
ExecStart=/usr/local/bin/hyperkube apiserver \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_ETCD_SERVERS \
            $KUBE_API_ADDRESS \
            $KUBE_API_PORT \
            $KUBELET_PORT \
            $KUBE_ALLOW_PRIV \
            $KUBE_SERVICE_ADDRESSES \
            $KUBE_ADMISSION_CONTROL \
            $KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;kube-controller-managerservice&quot;&gt;kube-controller-manager.service&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
User=root
ExecStart=/usr/local/bin/hyperkube controller-manager \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;kubeletservice&quot;&gt;kubelet.service&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service
[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/local/bin/hyperkube kubelet \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBELET_API_SERVER \
            $KUBELET_ADDRESS \
            $KUBELET_PORT \
            $KUBELET_HOSTNAME \
            $KUBE_ALLOW_PRIV \
            $KUBELET_ARGS
Restart=on-failure
KillMode=process
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;kube-proxyservice&quot;&gt;kube-proxy.service&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/proxy
ExecStart=/usr/local/bin/hyperkube proxy \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_PROXY_ARGS
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;kube-schedulerservice&quot;&gt;kube-scheduler.service&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
User=root
ExecStart=/usr/local/bin/hyperkube scheduler \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;43-配置文件&quot;&gt;4.3 配置文件&lt;/h3&gt;
&lt;h4 id=&quot;431-master-节点配置&quot;&gt;4.3.1 master 节点配置&lt;/h4&gt;
&lt;p&gt;Master 节点主要会运行 3 各组件: kube-apiserver、kube-controller-manager、kube-scheduler，其中用到的配置文件如下&lt;/p&gt;
&lt;h5 id=&quot;config&quot;&gt;config&lt;/h5&gt;
&lt;p&gt;config 是一个通用配置文件，值得注意的是由于安装时对于 Node、Master 节点都会包含该文件;
在 Node 节点上请注释掉 KUBE_MASTER 变量，因为 Node 节点需要做 HA，要连接本地的 6443 加密端口；而这个变量将会覆盖 kubeconfig 中指定的 127.0.0.1:6443 地址&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;

# journal message level, 0 is debug
KUBE_LOG_LEVEL=&quot;--v=2&quot;

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV=&quot;--allow-privileged=true&quot;

# How the controller-manager, scheduler, and proxy find the apiserver
KUBE_MASTER=&quot;--master=http://127.0.0.1:8080&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;apiserver&quot;&gt;apiserver&lt;/h5&gt;
&lt;p&gt;apiserver 配置相对于 1.8 略有变动，其中准入控制器(admission control)选项名称变为了 –enable-admission-plugins，控制器列表也有相应变化，这里采用官方推荐配置，具体请参考 &lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use&quot;&gt;官方文档&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;###
# kubernetes system config
#
# The following values are used to configure the kube-apiserver
#
# The address on the local server to listen to.
KUBE_API_ADDRESS=&quot;--advertise-address=192.168.2.219 --bind-address=192.168.2.219&quot;
# The port on the local server to listen on.
KUBE_API_PORT=&quot;--secure-port=6443&quot;
# Port minions listen on
# KUBELET_PORT=&quot;--kubelet-port=10250&quot;
# Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS=&quot;--etcd-servers=https://192.168.2.219:2379,https://192.168.2.220:2379,https://192.168.2.221:2379&quot;
# Address range to use for services
KUBE_SERVICE_ADDRESSES=&quot;--service-cluster-ip-range=10.254.0.0/16&quot;
# default admission control policies
KUBE_ADMISSION_CONTROL=&quot;--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction&quot;
# Add your own!
KUBE_API_ARGS=&quot; --anonymous-auth=false \
                --apiserver-count=3 \
                --audit-log-maxage=30 \
                --audit-log-maxbackup=3 \
                --audit-log-maxsize=100 \
                --audit-log-path=/var/log/kube-audit/audit.log \
                --audit-policy-file=/etc/kubernetes/audit-policy.yaml \
                --authorization-mode=Node,RBAC \
                --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \
                --enable-bootstrap-token-auth \
                --enable-garbage-collector \
                --enable-logs-handler \
                --enable-swagger-ui \
                --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \
                --etcd-certfile=/etc/etcd/ssl/etcd.pem \
                --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \
                --etcd-compaction-interval=5m0s \
                --etcd-count-metric-poll-period=1m0s \
                --event-ttl=48h0m0s \
                --kubelet-https=true \
                --kubelet-timeout=3s \
                --log-flush-frequency=5s \
                --token-auth-file=/etc/kubernetes/token.csv \
                --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem \
                --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \
                --service-node-port-range=30000-50000 \
                --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \
                --storage-backend=etcd3 \
                --enable-swagger-ui=true&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;controller-manager&quot;&gt;controller-manager&lt;/h5&gt;
&lt;p&gt;controller manager 配置默认开启了证书轮换能力用于自动签署 kueblet 证书，并且证书时间也设置了 10 年，可自行调整；增加了 –controllers 选项以指定开启全部控制器&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;###
# The following values are used to configure the kubernetes controller-manager
# defaults from config and apiserver should be adequate
# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS=&quot;  --bind-address=0.0.0.0 \
                                --cluster-name=kubernetes \
                                --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \
                                --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \
                                --controllers=*,bootstrapsigner,tokencleaner \
                                --deployment-controller-sync-period=10s \
                                --experimental-cluster-signing-duration=86700h0m0s \
                                --leader-elect=true \
                                --node-monitor-grace-period=40s \
                                --node-monitor-period=5s \
                                --pod-eviction-timeout=5m0s \
                                --terminated-pod-gc-threshold=50 \
                                --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \
                                --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \
                                --feature-gates=RotateKubeletServerCertificate=true&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;scheduler&quot;&gt;scheduler&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;###
# kubernetes scheduler config

# default config should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS=&quot;   --address=0.0.0.0 \
                        --leader-elect=true \
                        --algorithm-provider=DefaultProvider&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;432-node-节点配置&quot;&gt;4.3.2 node 节点配置&lt;/h4&gt;
&lt;p&gt;Node 节点上主要有 kubelet、kube-proxy 组件，用到的配置如下&lt;/p&gt;

&lt;h5 id=&quot;kubelet&quot;&gt;kubelet&lt;/h5&gt;
&lt;p&gt;kubeket 默认也开启了证书轮换能力以保证自动续签相关证书，同时增加了 –node-labels 选项为 node 打一个标签，关于这个标签最后部分会有讨论，如果在 master 上启动 kubelet，请将 node-role.kubernetes.io/k8s-node=true 修改为 node-role.kubernetes.io/k8s-master=true&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;###
# kubernetes kubelet (minion) config
# The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)
KUBELET_ADDRESS=&quot;--node-ip=192.168.2.219&quot;
# The port for the info server to serve on
# KUBELET_PORT=&quot;--port=10250&quot;
# You may leave this blank to use the actual hostname
KUBELET_HOSTNAME=&quot;--hostname-override=master219&quot;
# location of the api-server
# KUBELET_API_SERVER=&quot;&quot;
# Add your own!
KUBELET_ARGS=&quot;  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \
                --cert-dir=/etc/kubernetes/ssl \
                --cgroup-driver=cgroupfs \
                --cluster-dns=10.254.0.2 \
                --cluster-domain=cluster.local. \
                --fail-swap-on=false \
                --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \
                --node-labels=node-role.kubernetes.io/master=true \
                --image-gc-high-threshold=70 \
                --image-gc-low-threshold=50 \
                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \
                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
                --system-reserved=cpu=1000m,memory=1024Mi,ephemeral-storage=1Gi \
                --serialize-image-pulls=false \
                --sync-frequency=30s \
                --pod-infra-container-image=k8s.gcr.io/pause-amd64:3.0 \
                --resolv-conf=/etc/resolv.conf \
                --rotate-certificates&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;proxy&quot;&gt;proxy&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;###
# kubernetes proxy config
# default config should be adequate
# Add your own!
KUBE_PROXY_ARGS=&quot;--bind-address=0.0.0.0 \
                 --hostname-override=node219 \
                 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
                 --cluster-cidr=10.254.0.0/16&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;44-启动-master-节点&quot;&gt;4.4 启动 Master 节点&lt;/h3&gt;
&lt;p&gt;创建相关目录否则报错&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir /var/log/kube-audit
mkdir /var/lib/kubelet
mkdir /usr/libexec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cp -a conf /etc/kubernetes  
cp systemd/*.service /lib/systemd/system
systemctl daemon-reload
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;对于 &lt;code class=&quot;highlighter-rouge&quot;&gt;master&lt;/code&gt; 节点启动无需做过多处理，多个 &lt;code class=&quot;highlighter-rouge&quot;&gt;master&lt;/code&gt; 只要保证 &lt;code class=&quot;highlighter-rouge&quot;&gt;apiserver&lt;/code&gt; 等配置中的 &lt;code class=&quot;highlighter-rouge&quot;&gt;ip&lt;/code&gt; 地址监听没问题后直接启动即可&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl daemon-reload
systemctl start kube-apiserver
systemctl start kube-controller-manager
systemctl start kube-scheduler
systemctl enable kube-apiserver
systemctl enable kube-controller-manager
systemctl enable kube-scheduler

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;启动后:
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/kubernetes/kubernetes-get-cs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;45-启动-node-节点&quot;&gt;4.5 启动 Node 节点&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;由于 HA 等功能需要，对于 Node 需要做一些处理才能启动，主要有以下两个地方需要处理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;451-nginx-proxy-node节点部署&quot;&gt;4.5.1 nginx-proxy (node节点部署)&lt;/h4&gt;
&lt;p&gt;在启动 kubelet、kube-proxy 服务之前，需要在本地启动 nginx 来 tcp 负载均衡 apiserver 6443 端口，nginx-proxy 使用 docker + systemd 启动，配置如下&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;nginx-proxy.service&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Unit]
Description=kubernetes apiserver docker wrapper
Wants=docker.socket
After=docker.service

[Service]
User=root
PermissionsStartOnly=true
ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \
                              -v /etc/nginx:/etc/nginx \
                              --name nginx-proxy \
                              --net=host \
                              --restart=on-failure:5 \
                              --memory=512M \
                              nginx:1.13.12-alpine
ExecStartPre=-/usr/bin/docker rm -f nginx-proxy
ExecStop=/usr/bin/docker stop nginx-proxy
Restart=always
RestartSec=15s
TimeoutStartSec=30s

[Install]
WantedBy=multi-user.target

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;nginx.conf&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;error_log stderr notice;

worker_processes auto;
events {
        multi_accept on;
        use epoll;
        worker_connections 1024;
}

stream {
    upstream kube_apiserver {
        least_conn;
        server 192.168.2.219:6443; ## 多个master 依次配置即可
        # server x.x.x.x:6443;
    }

    server {
        listen        0.0.0.0:6443;
        proxy_pass    kube_apiserver;
        proxy_timeout 10m;
        proxy_connect_timeout 1s;
    }
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;启动 apiserver 的本地负载均衡&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir /etc/nginx
cp nginx.conf /etc/nginx
cp nginx-proxy.service /lib/systemd/system

systemctl daemon-reload
systemctl start nginx-proxy
systemctl enable nginx-proxy

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;452-tls-bootstrapping&quot;&gt;4.5.2 TLS bootstrapping&lt;/h4&gt;
&lt;p&gt;创建好 nginx-proxy 后不要忘记为 TLS Bootstrap 创建相应的 RBAC 规则，这些规则能实现证自动签署 TLS Bootstrap 发出的 CSR 请求，从而实现证书轮换(创建一次即可);&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;tls-bootstrapping-clusterrole.yaml&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# A ClusterRole which instructs the CSR approver to approve a node requesting a
# serving cert matching its client cert.
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver
rules:
- apiGroups: [&quot;certificates.k8s.io&quot;]
  resources: [&quot;certificatesigningrequests/selfnodeserver&quot;]
  verbs: [&quot;create&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在 master 执行创建&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 给与 kubelet-bootstrap 用户进行 node-bootstrapper 的权限
kubectl create clusterrolebinding kubelet-bootstrap \
    --clusterrole=system:node-bootstrapper \
    --user=kubelet-bootstrap

kubectl create -f tls-bootstrapping-clusterrole.yaml

# 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求
kubectl create clusterrolebinding node-client-auto-approve-csr \
        --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient \
        --group=system:bootstrappers

# 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求
kubectl create clusterrolebinding node-client-auto-renew-crt \
        --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \
        --group=system:nodes

# 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求
kubectl create clusterrolebinding node-server-auto-renew-crt \
        --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver \
        --group=system:nodes

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;453-启动&quot;&gt;4.5.3 启动&lt;/h4&gt;
&lt;p&gt;多节点部署时先启动好 nginx-proxy，然后修改好相应配置的 ip 地址等配置，最终直接启动即可(master 上启动 kubelet 不要忘了修改 kubeconfig 中的 apiserver 地址，还有对应的 kubelet 的 node label)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl daemon-reload
systemctl start kubelet
systemctl start kube-proxy
systemctl enable kubelet
systemctl enable kube-proxy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;454-master-节点启动-kubelet&quot;&gt;4.5.4 master 节点启动 kubelet&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;注意: 对于在 master 节点启动 kubelet 来说，只需要修改 kubelet.kubeconfig、kube-proxy.kubeconfig 中的 apiserver 地址为当前 master ip 6443 端口即可&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# grep server kube-proxy.kubeconfig
server: https://192.168.2.219:6443
# grep server bootstrap.kubeconfig
server: https://192.168.2.219:6443

systemctl daemon-reload
systemctl start kubelet
systemctl start kube-proxy
systemctl enable kubelet
systemctl enable kube-proxy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;最终成功启动后:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/kubernetes/kubernetes-get-node.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;五-calico&quot;&gt;五. Calico&lt;/h2&gt;
&lt;h3 id=&quot;51-修改calico-配置&quot;&gt;5.1 修改calico 配置&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#wget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yaml -O calico.example.yaml

ETCD_CERT=`cat /etc/etcd/ssl/etcd.pem | base64 | tr -d '\n'`
ETCD_KEY=`cat /etc/etcd/ssl/etcd-key.pem | base64 | tr -d '\n'`
ETCD_CA=`cat /etc/etcd/ssl/etcd-root-ca.pem | base64 | tr -d '\n'`
ETCD_ENDPOINTS=&quot;https://192.168.2.219:2379,https://192.168.2.220:2379,https://192.168.2.221:2379&quot;

cp calico.example.yaml calico.yaml

sed -i &quot;s@.*etcd_endpoints:.*@\ \ etcd_endpoints:\ \&quot;${ETCD_ENDPOINTS}\&quot;@gi&quot; calico.yaml

sed -i &quot;s@.*etcd-cert:.*@\ \ etcd-cert:\ ${ETCD_CERT}@gi&quot; calico.yaml
sed -i &quot;s@.*etcd-key:.*@\ \ etcd-key:\ ${ETCD_KEY}@gi&quot; calico.yaml
sed -i &quot;s@.*etcd-ca:.*@\ \ etcd-ca:\ ${ETCD_CA}@gi&quot; calico.yaml

sed -i 's@.*etcd_ca:.*@\ \ etcd_ca:\ &quot;/calico-secrets/etcd-ca&quot;@gi' calico.yaml
sed -i 's@.*etcd_cert:.*@\ \ etcd_cert:\ &quot;/calico-secrets/etcd-cert&quot;@gi' calico.yaml
sed -i 's@.*etcd_key:.*@\ \ etcd_key:\ &quot;/calico-secrets/etcd-key&quot;@gi' calico.yaml

# 注释掉 calico-node 部分(由 Systemd 接管)
sed -i '123,219s@.*@#&amp;amp;@gi' calico.yaml

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;52-创建systemd-文件&quot;&gt;5.2 创建systemd 文件&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;需要在每个节点(master和node)执行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;K8S_MASTER_IP=&quot;192.168.2.219&quot;
HOSTNAME=`cat /etc/hostname`
ETCD_ENDPOINTS=&quot;https://192.168.2.219:2379,https://192.168.2.220:2379,https://192.168.2.221:2379&quot;

cat &amp;gt; /lib/systemd/system/calico-node.service &amp;lt;&amp;lt;EOF
[Unit]
Description=calico node
After=docker.service
Requires=docker.service

[Service]
User=root
Environment=ETCD_ENDPOINTS=${ETCD_ENDPOINTS}
PermissionsStartOnly=true
ExecStart=/usr/bin/docker run   --net=host --privileged --name=calico-node \\
                                -e ETCD_ENDPOINTS=${ETCD_ENDPOINTS} \\
                                -e ETCD_CA_CERT_FILE=/etc/etcd/ssl/etcd-root-ca.pem \\
                                -e ETCD_CERT_FILE=/etc/etcd/ssl/etcd.pem \\
                                -e ETCD_KEY_FILE=/etc/etcd/ssl/etcd-key.pem \\
                                -e NODENAME=${HOSTNAME} \\
                                -e IP= \\
                                -e IP_AUTODETECTION_METHOD=can-reach=${K8S_MASTER_IP} \\
                                -e AS=64512 \\
                                -e CLUSTER_TYPE=k8s,bgp \\
                                -e CALICO_IPV4POOL_CIDR=10.20.0.0/16 \\
                                -e CALICO_IPV4POOL_IPIP=always \\
                                -e CALICO_LIBNETWORK_ENABLED=true \\
                                -e CALICO_NETWORKING_BACKEND=bird \\
                                -e CALICO_DISABLE_FILE_LOGGING=true \\
                                -e FELIX_IPV6SUPPORT=false \\
                                -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \\
                                -e FELIX_LOGSEVERITYSCREEN=info \\
                                -e FELIX_IPINIPMTU=1440 \\
                                -e FELIX_HEALTHENABLED=true \\
                                -e CALICO_K8S_NODE_REF=${HOSTNAME} \\
                                -v /etc/calico/ssl:/etc/etcd/ssl \\
                                -v /lib/modules:/lib/modules \\
                                -v /var/lib/calico:/var/lib/calico \\
                                -v /var/run/calico:/var/run/calico \\
                                k8s.yfcloud.com/calico/node:v3.1.0
ExecStop=/usr/bin/docker rm -f calico-node
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;对于以上脚本中的 K8S_MASTER_IP 变量，只需要填写一个 master ip 即可，这个变量用于 calico 自动选择 IP 使用；在宿主机有多张网卡的情况下，calcio node 会自动获取一个 IP，获取原则就是尝试是否能够联通这个 master ip&lt;/p&gt;

&lt;p&gt;由于 calico 需要使用 etcd 存储数据，所以需要复制 etcd 证书到相关目录，/etc/calico 需要在每个节点都有&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cp -r /etc/etcd/ssl /etc/calico
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;53-修改-kubelet-配置&quot;&gt;5.3 修改 kubelet 配置&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;使用 Calico 后需要修改 kubelet 配置增加 CNI 设置(–network-plugin=cni)，修改后配置如下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;###
# kubernetes kubelet (minion) config
# The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)
KUBELET_ADDRESS=&quot;--node-ip=192.168.2.219&quot;
# The port for the info server to serve on
# KUBELET_PORT=&quot;--port=10250&quot;
# You may leave this blank to use the actual hostname
KUBELET_HOSTNAME=&quot;--hostname-override=master219&quot;
# location of the api-server
# KUBELET_API_SERVER=&quot;&quot;
# Add your own!
KUBELET_ARGS=&quot;  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \
                --cert-dir=/etc/kubernetes/ssl \
                --cgroup-driver=cgroupfs \
                --cluster-dns=10.254.0.2 \
                --network-plugin=cni \
                --cluster-domain=cluster.local. \
                --fail-swap-on=false \
                --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \
                --node-labels=node-role.kubernetes.io/master=true \
                --image-gc-high-threshold=70 \
                --image-gc-low-threshold=50 \
                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \
                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
                --system-reserved=cpu=1000m,memory=1024Mi,ephemeral-storage=1Gi \
                --serialize-image-pulls=false \
                --sync-frequency=30s \
                --pod-infra-container-image=k8s.gcr.io/pause-amd64:3.0 \
                --resolv-conf=/etc/resolv.conf \
                --rotate-certificates&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;54-创建-calico-daemonset&quot;&gt;5.4 创建 Calico Daemonset&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 先创建 RBAC
kubectl apply -f \
https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/rbac.yaml

# 再创建 Calico Daemonset
kubectl create -f calico.yaml

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;55-启动-calico-node&quot;&gt;5.5 启动 Calico Node&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl daemon-reload
systemctl restart calico-node
systemctl enable calico-node

# 等待 20s 拉取镜像, 可以提前将镜像拉取
sleep 20
systemctl restart kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;56-测试网络&quot;&gt;5.6 测试网络&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 创建 deployment
cat &amp;lt;&amp;lt; EOF &amp;gt;&amp;gt; demo.deploy.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      app: demo
  template:
    metadata:
      labels:
        app: demo
    spec:
      containers:
      - name: demo
        image: jevic/demo
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
EOF
kubectl create -f demo.deploy.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;测试结果&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@master219 ~]# kubectl get pod -o wide
NAME                             READY     STATUS    RESTARTS   AGE       IP             NODE
demo-deployment-566fd7b7-fjg7p   1/1       Running   0          3d        10.20.211.67   node220
demo-deployment-566fd7b7-qz5q2   1/1       Running   0          3d        10.20.237.5    master219
demo-deployment-566fd7b7-zzqf9   1/1       Running   0          3d        10.20.206.3    node221
[root@master219 ~]# kubectl exec -it demo-deployment-566fd7b7-fjg7p ping 10.20.206.3
PING 10.20.206.3 (10.20.206.3): 56 data bytes
64 bytes from 10.20.206.3: seq=0 ttl=62 time=0.431 ms
64 bytes from 10.20.206.3: seq=1 ttl=62 time=0.257 ms
64 bytes from 10.20.206.3: seq=2 ttl=62 time=0.246 ms
64 bytes from 10.20.206.3: seq=3 ttl=62 time=0.337 ms
^C
--- 10.20.206.3 ping statistics ---
4 packets transmitted, 4 packets received, 0% packet loss
round-trip min/avg/max = 0.246/0.317/0.431 ms
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;六-部署dns&quot;&gt;六. 部署DNS&lt;/h2&gt;
&lt;h3 id=&quot;61-修改配置文件&quot;&gt;6.1 修改配置文件&lt;/h3&gt;
&lt;p&gt;将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。
coredns 对应的目录是：&lt;code class=&quot;highlighter-rouge&quot;&gt;cluster/addons/dns&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# diff coredns.yaml coredns.yaml.base
61c61
&amp;lt;         kubernetes cluster.local. in-addr.arpa ip6.arpa {
---
&amp;gt;         kubernetes __PILLAR__DNS__DOMAIN__ in-addr.arpa ip6.arpa {
103c103
&amp;lt;         image: k8s.yfcloud.com/k8s.gcr.io/coredns:1.0.6
---
&amp;gt;         image: k8s.gcr.io/coredns:1.0.6
153c153
&amp;lt;   clusterIP: 10.254.0.2
---
&amp;gt;   clusterIP: __PILLAR__DNS__SERVER__
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;62-检查coredns&quot;&gt;6.2 检查coredns&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# kubectl get pod -n kube-system  |grep dns
coredns-787558c684-6tlwj                   1/1       Running   0          3d
coredns-787558c684-t4h6j                   1/1       Running   0          3d

# kubectl exec -it demo-deployment-566fd7b7-qz5q2 bash
bash-4.4# nslookup kubernetes
nslookup: can't resolve '(null)': Name does not resolve

Name:      kubernetes
Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;63-dns-自动扩容&quot;&gt;6.3 DNS 自动扩容&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.10.md#downloads-for-v11010&quot;&gt;下载源码文件&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;文件路径: kubernetes/kubernetes-src1.10.1/cluster/addons/dns-horizontal-autoscaler&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f dns-horizontal-autoscaler.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;七-dashboard&quot;&gt;七. Dashboard&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml -O kubernetes-dashboard.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;71-配置端口&quot;&gt;7.1 配置端口&lt;/h3&gt;

&lt;p&gt;便于访问配置为: NodePort, 最后的修改部分如下&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# ------------------- Dashboard Service ------------------- #

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  type: NodePort
  ports:
    - name: dashboard-tls
      port: 443
      targetPort: 8443
      nodePort: 30000
      protocol: TCP
  selector:
    k8s-app: kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;执行 kubectl create -f kubernetes-dashboard.yaml 创建即可&lt;/p&gt;

&lt;h3 id=&quot;72-创建admin-账户&quot;&gt;7.2 创建Admin 账户&lt;/h3&gt;

&lt;p&gt;默认情况下部署成功后可以直接访问 https://NODE_IP:30000 访问，但是想要登录进去查看的话需要使用 kubeconfig 或者 access token 的方式；实际上这个就是 RBAC 授权控制，以下提供一个创建 admin access token 的脚本，更细节的权限控制比如只读用户可以参考&lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding&quot;&gt;官方文档&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;kubectl get sa dashboard-admin &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system &amp;amp;&amp;gt; /dev/null&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;then
    &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\0&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;33[33mWARNING: ServiceAccount dashboard-admin exist!&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\0&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;33[0m&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else
    &lt;/span&gt;kubectl create sa dashboard-admin &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system
    kubectl create clusterrolebinding dashboard-admin &lt;span class=&quot;nt&quot;&gt;--clusterrole&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cluster-admin &lt;span class=&quot;nt&quot;&gt;--serviceaccount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-system:dashboard-admin
&lt;span class=&quot;k&quot;&gt;fi

&lt;/span&gt;kubectl describe secret &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;kubectl get secrets &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;dashboard-admin | cut &lt;span class=&quot;nt&quot;&gt;-f1&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-E&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'^token'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;成功后访问如下(如果访问不了的话请检查下 iptable FORWARD 默认规则是否为 DROP，如果是将其改为 ACCEPT 即可)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/kubernetes/kubernetes-dashboard-sa.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/kubernetes/kubernetes-dashboard.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;八-部署-heapster&quot;&gt;八. 部署 heapster&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md&quot;&gt;官方说明&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;git clone https://github.com/kubernetes/heapster.git&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;81-grafana-nodeport&quot;&gt;8.1 grafana NodePort&lt;/h3&gt;
&lt;p&gt;在&lt;a href=&quot;https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md&quot;&gt;官方部署说明&lt;/a&gt;中已经有提示,需要将grafana改为NodePort 进而方可使用各节点IP:Port访问，下面列出修改后的配置&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.... 省略上面部分 .....
apiVersion: v1
kind: Service
metadata:
  labels:
    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)
    # If you are NOT using this as an addon, you should comment out this line.
    kubernetes.io/cluster-service: 'true'
    kubernetes.io/name: monitoring-grafana
  name: monitoring-grafana
  namespace: kube-system
spec:
  # In a production setup, we recommend accessing Grafana through an external Loadbalancer
  # or through a public IP.
  # type: LoadBalancer
  # You could also use NodePort to expose the service at a randomly-generated port
  type: NodePort
  ports:
  - name: grafana-web  ## 自定义
    port: 80
    targetPort: 3000
    nodePort: 30001 ## 30000-50000 自定义
    protocol: TCP
  selector:
    k8s-app: grafana
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;82-执行部署&quot;&gt;8.2 执行部署&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f deploy/kube-config/influxdb/
kubectl create -f deploy/kube-config/rbac/heapster-rbac.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;83-查看grafana&quot;&gt;8.3 查看grafana&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;面板需要自己手动添加&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/kubernetes/kubernetes-grafana.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;84-访问dashboard&quot;&gt;8.4 访问Dashboard&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;可以看到界面上面已经显示内存 CPU资源图表&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/kubernetes/kubernetes-dashboard-view01.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;九-其他说明&quot;&gt;九. 其他说明:&lt;/h2&gt;
&lt;h3 id=&quot;91-coredns&quot;&gt;9.1 coredns&lt;/h3&gt;

&lt;p&gt;如果配置失败或者报错请检查 /etc/resolv.conf 域名解析配置&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat /etc/resolv.conf
# Generated by NetworkManager
#search lan
#nameserver 192.168.1.1
nameserver 223.5.5.5
nameserver 8.8.8.8
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;gcr-镜像被墙&quot;&gt;GCR 镜像被墙&lt;/h3&gt;
&lt;p&gt;默认kubernetes 镜像被托管在Google仓库无法被正常获取,使用开源社区镜像仓库Pull 即可
参考&lt;a href=&quot;https://www.jevic.cn/2018/05/25/mirror/&quot;&gt;GCR Google Container Registry 镜像&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;dashboard&quot;&gt;dashboard&lt;/h3&gt;
&lt;p&gt;直接使用kubernetes-src 里面的yaml文件创建即可,记得修改NodePort&lt;/p&gt;

&lt;h3 id=&quot;heapster&quot;&gt;heapster&lt;/h3&gt;
&lt;p&gt;同样直接使用github 官方提供的说明配置即可, 其他部署方式都不靠谱&lt;/p&gt;</content><author><name>Jevic</name></author><summary type="html">通读一遍在实际操作!!! 关于镜像请查看最后的补充说明 部署脚本查看仓库脚本</summary></entry><entry><title type="html">elasticsearch templates 模板配置</title><link href="http://0.0.0.0/2018/08/25/elasticsearch-templates/" rel="alternate" type="text/html" title="elasticsearch templates 模板配置" /><published>2018-08-25T16:42:16+08:00</published><updated>2018-08-25T16:42:16+08:00</updated><id>http://0.0.0.0/2018/08/25/elasticsearch-templates</id><content type="html" xml:base="http://0.0.0.0/2018/08/25/elasticsearch-templates/">&lt;h2 id=&quot;动态模板加载&quot;&gt;动态模板加载&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;order&quot;: 0,
  &quot;template&quot;: &quot;api-*&quot;,
  &quot;settings&quot;: {
    &quot;index&quot;: {
      &quot;number_of_shards&quot;: &quot;2&quot;,
      &quot;number_of_replicas&quot;: &quot;1&quot;,
      &quot;refresh_interval&quot;: &quot;60s&quot;
    }
  },
  &quot;mappings&quot;: {
    &quot;_default_&quot;: {
      &quot;dynamic_templates&quot;: [
        {
          &quot;message_field&quot;: {
            &quot;mapping&quot;: {
              &quot;fielddata&quot;: {
                &quot;format&quot;: &quot;disabled&quot;
              },
              &quot;index&quot;: &quot;not_analyzed&quot;,
              &quot;omit_norms&quot;: true,
              &quot;type&quot;: &quot;string&quot;
            },
            &quot;match_mapping_type&quot;: &quot;string&quot;,
            &quot;match&quot;: &quot;message&quot;
          }
        },
        {
          &quot;string_fields&quot;: {
            &quot;mapping&quot;: {
              &quot;fielddata&quot;: {
                &quot;format&quot;: &quot;disabled&quot;
              },
              &quot;index&quot;: &quot;not_analyzed&quot;,
              &quot;omit_norms&quot;: true,
              &quot;type&quot;: &quot;string&quot;,
              &quot;fields&quot;: {
                &quot;raw&quot;: {
                  &quot;ignore_above&quot;: 256,
                  &quot;index&quot;: &quot;not_analyzed&quot;,
                  &quot;type&quot;: &quot;string&quot;
                }
              }
            },
            &quot;match_mapping_type&quot;: &quot;string&quot;,
            &quot;match&quot;: &quot;*&quot;
          }
        }
      ],
      &quot;_all&quot;: {
        &quot;omit_norms&quot;: true,
        &quot;enabled&quot;: false
      },
      &quot;properties&quot;: {
        &quot;args&quot;: {
          &quot;index&quot;: &quot;not_analyzed&quot;,
          &quot;type&quot;: &quot;string&quot;
        },
        &quot;@timestamp&quot;: {
          &quot;type&quot;: &quot;date&quot;
        },
        &quot;request_time&quot;: {
          &quot;index&quot;: &quot;not_analyzed&quot;,
          &quot;type&quot;: &quot;float&quot;
        },
        &quot;status&quot;: {
          &quot;index&quot;: &quot;not_analyzed&quot;,
          &quot;type&quot;: &quot;long&quot;
        }
      }
    }
  },
  &quot;aliases&quot;: {}
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;mapping&quot;&gt;mapping&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;适用于&amp;lt;6.x 版本&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;order&quot;: 0,
  &quot;template&quot;: &quot;test-*&quot;,
  &quot;settings&quot;: {
    &quot;index&quot;: {
      &quot;number_of_shards&quot;: &quot;3&quot;,
      &quot;number_of_replicas&quot;: &quot;1&quot;,
      &quot;refresh_interval&quot;: &quot;60s&quot;
    }
  },
  &quot;mappings&quot;: {
    &quot;test1&quot;: {
      &quot;properties&quot;: {
        &quot;ext&quot;: {
          &quot;index&quot;: &quot;not_analyzed&quot;,
          &quot;type&quot;: &quot;string&quot;
        },
        &quot;id&quot;: {
          &quot;index&quot;: &quot;not_analyzed&quot;,
          &quot;type&quot;: &quot;string&quot;
        }
      }
    },
    &quot;test2&quot;: {
      &quot;properties&quot;: {
        &quot;ext&quot;: {
          &quot;index&quot;: &quot;not_analyzed&quot;,
          &quot;type&quot;: &quot;string&quot;
        },
        &quot;status&quot;: {
          &quot;index&quot;: &quot;not_analyzed&quot;,
          &quot;type&quot;: &quot;long&quot;
        }
      }
    }
  },
  &quot;aliases&quot;: {}
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;转载请注明出处，本文采用 &lt;a href=&quot;http://creativecommons.org/licenses/by-nc-nd/4.0/&quot;&gt;CC4.0&lt;/a&gt; 协议授权&lt;/p&gt;</content><author><name>Jevic</name></author><summary type="html">动态模板加载</summary></entry><entry><title type="html">Kubeadm 快速部署kubernetes 1.10.1</title><link href="http://0.0.0.0/2018/07/17/kubeadm-kubernetes1.10.1/" rel="alternate" type="text/html" title="Kubeadm 快速部署kubernetes 1.10.1" /><published>2018-07-17T20:35:46+08:00</published><updated>2018-07-17T20:35:46+08:00</updated><id>http://0.0.0.0/2018/07/17/kubeadm-kubernetes1.10.1</id><content type="html" xml:base="http://0.0.0.0/2018/07/17/kubeadm-kubernetes1.10.1/">&lt;h2 id=&quot;系统环境&quot;&gt;系统环境&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;IP地址&lt;/th&gt;
      &lt;th&gt;主机名&lt;/th&gt;
      &lt;th&gt;Docker 版本&lt;/th&gt;
      &lt;th&gt;kubernetes 版本&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;192.168.2.65&lt;/td&gt;
      &lt;td&gt;k1.master&lt;/td&gt;
      &lt;td&gt;17.03.1-ce&lt;/td&gt;
      &lt;td&gt;v1.10.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;192.168.2.66&lt;/td&gt;
      &lt;td&gt;k2.master&lt;/td&gt;
      &lt;td&gt;17.03.1-ce&lt;/td&gt;
      &lt;td&gt;v1.10.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;192.168.2.67&lt;/td&gt;
      &lt;td&gt;k3.master&lt;/td&gt;
      &lt;td&gt;17.03.1-ce&lt;/td&gt;
      &lt;td&gt;v1.10.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;初始化&quot;&gt;初始化&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;所有节点执行&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@k1 ~]# setenforce 0
[root@k1 ~]# sed -i 's/SELINUX=enforcing/SELINUX=disalbe/g' /etc/sysconfig/selinux

[root@k1 ~]# cat &amp;gt;&amp;gt; /etc/security/limits.conf &amp;lt;&amp;lt;EOF
* soft nofile 65536
* hard nofile 65536
* soft nproc unlimited
* hard nproc unlimited
EOF

[root@k1 ~]# swapoff -a

[root@k1 ~]# cat &amp;gt;&amp;gt; /etc/sysctl.conf &amp;lt;&amp;lt;EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
net.ipv4.tcp_tw_recycle=0
vm.swappiness=0
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.ipv6.conf.all.disable_ipv6=1
net.netfilter.nf_conntrack_max=2310720
EOF

[root@k1 ~]# systemctl stop firewalld &amp;amp;&amp;amp; systemctl disable firewalld

[root@k1 ~]# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo

[root@k1 ~]# yum makecache fast
[root@k1 ~]# yum install -y epel-release
[root@k1 ~]# yum install -y conntrack ipvsadm ipset jq sysstat curl iptables libseccomp ntpdate
[root@k1 ~]# ntpdate cn.pool.ntp.org
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;install-docker&quot;&gt;Install Docker&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;参考 &lt;a href=&quot;https://docs.docker.com/install/linux/docker-ce/centos/#install-using-the-repository&quot;&gt;官方文档&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@k1 ~]# yum install -y yum-utils device-mapper-persistent-data lvm2
[root@k1 ~]# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
[root@k1 ~]# yum list docker-ce --showduplicates | sort -r
[root@k1 ~]# yum -y install docker-ce-17.03.1.ce
[root@k1 ~]# systemctl start docker &amp;amp;&amp;amp; systemctl stop docker
[root@k1 ~]# cat &amp;gt; /etc/docker/daemon.json &amp;lt;&amp;lt;EOF
{
  &quot;registry-mirrors&quot;: [&quot;https://dlvqhrac.mirror.aliyuncs.com&quot;]
}
EOF
[root@k1 ~]# systemctl daemon-reload
[root@k1 ~]# systemctl start docker

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;kubeadm-组件&quot;&gt;Kubeadm 组件&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;配置阿里云 yum 源&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@k1 ~]# cat &amp;gt; /etc/yum.repos.d/kubernetes.repo &amp;lt;&amp;lt;EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
enabled=1
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;安装组件&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@k1 ~]# yum install kubelet kubeadm kubectl

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;初始化-master&quot;&gt;初始化 Master&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;镜像
    &lt;ul&gt;
      &lt;li&gt;出于不可描述的原因获取镜像会遇到困难&lt;/li&gt;
      &lt;li&gt;从Docker hub FROM 官方镜像再从自己镜像仓库拉取即可&lt;/li&gt;
      &lt;li&gt;具体操作细节在此忽略&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;修改配置&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@k1 ~]# cat /etc/sysconfig/kubelet
KUBELET_EXTRA_ARGS=&quot;--fail-swap-on=false&quot;
KUBE_PROXY_MODE=ipvs,ip_vs,ip_vs_rr,ip_vs_wrr,ip_vs_sh,nf_conntrack_ipv4

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;初始化&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@k1 ~]# kubeadm init --kubernetes-version=v1.10.1 --pod-network-cidr=10.20.0.0/16 --apiserver-advertise-address=192.168.2.65

记录最后的节点初始化信息，添加节点即可 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;添加flannel网络插件
    &lt;ul&gt;
      &lt;li&gt;https://github.com/coreos/flannel&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;查看集群状态&quot;&gt;查看集群状态&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@k1 ~]# kubectl get node
NAME        STATUS   ROLES    AGE     VERSION
k1.master   Ready    master   2d14h   v1.10.1
k2.master   Ready    &amp;lt;none&amp;gt;   2d14h   v1.10.1
k3.master   Ready    &amp;lt;none&amp;gt;   2d14h   v1.10.1

[root@k1 ~]# kubectl get pod -n kube-system
NAME                                READY   STATUS             RESTARTS   AGE
etcd-k1.master                      1/1     Running            2          2d14h
kube-apiserver-k1.master            1/1     Running            5          2d14h
kube-controller-manager-k1.master   1/1     Running            4          2d14h
kube-flannel-ds-amd64-9xzfl         1/1     Running            2          2d14h
kube-flannel-ds-amd64-krc4l         1/1     Running            1          2d14h
kube-flannel-ds-amd64-phhc7         1/1     Running            2          2d14h
kube-proxy-4vndk                    1/1     Running            3          2d14h
kube-proxy-f4dqb                    1/1     Running            2          2d14h
kube-proxy-s6p9z                    1/1     Running            2          2d14h
kube-scheduler-k1.master            1/1     Running            4          2d14h

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;转载请注明出处，本文采用 &lt;a href=&quot;http://creativecommons.org/licenses/by-nc-nd/4.0/&quot;&gt;CC4.0&lt;/a&gt; 协议授权&lt;/p&gt;</content><author><name>Jevic</name></author><summary type="html">系统环境</summary></entry><entry><title type="html">CDN 厂商选择三要素</title><link href="http://0.0.0.0/2018/06/10/cdn-choice/" rel="alternate" type="text/html" title="CDN 厂商选择三要素" /><published>2018-06-10T20:25:16+08:00</published><updated>2018-06-10T20:25:16+08:00</updated><id>http://0.0.0.0/2018/06/10/cdn-choice</id><content type="html" xml:base="http://0.0.0.0/2018/06/10/cdn-choice/">&lt;blockquote&gt;
  &lt;p&gt;作为技术决策者在选择使用 CDN 服务时最关心的三个问题是：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;一，哪家的 CDN 更快（速度快，用户体验好）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;二， 哪家 CDN 功能最全，即使现在用不到也不会给将来业务发展挖坑。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;三，怎么付费最划算。本文通过分析对国内 CDN 市场占有率靠前的十家服务商的网络环境和技术服务，希望给大家提供一些启发和建议。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.idcps.com/idc/china/cdn&quot;&gt;IDC 评述网&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;哪家的-cdn-更快&quot;&gt;哪家的 CDN 更快？&lt;/h2&gt;

&lt;p&gt;CDN 服务商经常引用独立第三方公司的拨测数据来证明自己的 CDN 服务更好。虽然这些数据在某个区域或时间段也许是准确的，实际却是盲人摸象，无法证明全时段和全网 CDN 服务的真实性能。也许从 CDN 服务商所处的网络环境和提供的技术功能入手，会是更科学和公平的对比方法。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;阿里云，腾讯云和网宿科技由于支持 HttpDNS 在技术上有领先优势，可以列为第一梯队。第二梯队的是百度云，蓝讯，Ucloud 和网易云。它们所在 AS 和两个以上运营商主干网 AS 相邻，也具有一定优势。剩下的金山云，七牛云和京东云排在第三梯队。金山云所在 AS 只和电信骨干网 AS 相连，使用其它运营商的用户访问其 CDN 节点理论上会相对电信的慢一些。七牛云和京东云其网络属于北京电信通的 AS，需要穿过两个 AS 才接入骨干网，理论上速度也会比其他 CDN 服务商稍慢。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;国内用户普遍使用互联网提供商 (ISP) 的宽带上网，具体访问流程如下图：
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/cdn_wx01.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;用户终端访问 CDN 的过程分两个步骤;
一是用户通过 DNS 找到最近的 CDN 边缘节点 IP; 
二是数据在网络中的送达用户终端。
整个过程中，有三个方面会影响用户访问 CDN 的体验。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;一拥有-dns-优化策略的-cdn-提供商会有更好的用户体验&quot;&gt;一，拥有 DNS 优化策略的 CDN 提供商，会有更好的用户体验。&lt;/h5&gt;

&lt;p&gt;从图 1 可见， 客户终端的 DNS Resolver 负责告诉浏览器到哪里去找 CDN 的资源。理论上 ISP 的 DNS 服务器会选择离用户最近 CDN 节点 IP 并返回给用户，但是实际情况并不是这么简单。国内的大城市的 ISP 业务，除了一些区域性的 ISP，基本被联通、电信和移动这样的大运营商所垄断。由于各运营商之间存在着网间费用结算，运营商会想尽一切办法将用户的访问在自己的网内解决掉。比如，广州联调宽带的用户想访问的内容在联通北京的 CDN 节点， 尽管在广东移动的 CDN 节点有用户想访问的资源，联通的 DNS 还是会返回联通北京 CDN 节点的 IP。&lt;/p&gt;

&lt;p&gt;另外，一些 ISP 为了节省网间流量，未经 CDN 服务商同意，自己针对一些 CDN 文件做了一层 CDN 缓存，通过“DNS 劫持”把用户访问 CDN 资源的请求都指到自己网内的非法 CDN 缓存服务器。很多时候这些缓存的内容不能及时和 CDN 节点同步更新，会造成使用该 ISP 的用户终端出现访问 CDN 资源缓慢，失败等现象。同时，国内严重的 DNS 污染问题也影响了用户的上网体验。&lt;/p&gt;

&lt;p&gt;因此，如果能使用一些技术优化用户 DNS 查询，会大幅度提高用户的体验。目前优化 DNS 的技术主要是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;HttpDNS ：客户端基于 Http 协议向 CDN 服务商指定的 DNS 服务器发送域名解析请求，从而避免 LocalDNS 造成的域名劫持和跨网访问;
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/cdn_wx02.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Http 302 跳转: CDN 厂商维护 CDN 域名 IP 库，根据用户访问终端的 IP 和 CDN 边缘节点的状态，选择最合适的 CDN 节点，发出 HTTP 的 302 返回码，将用户的请求跳转到合适的 CDN 边缘节点。例如腾讯的下载直通车就使用类似技术。
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/cdn_wx02.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;二拥有自治系统autonomous-system-as的-cdn-提供商数据包跨越最少的网络边界能获得更快的传输速度&quot;&gt;二，拥有自治系统（Autonomous system, AS）的 CDN 提供商，数据包跨越最少的网络边界，能获得更快的传输速度。&lt;/h5&gt;

&lt;p&gt;在 BGP 协议中，IP 包从一个 AS 向另一个 AS 传输时，需要经过边界路由器，如果由于网络问题造成 IP 包不可达，则需要边界路由器重新规划线路。如果 CDN 服务商自己拥有自治系统，AS 内部拥有同样的选路策略，数据就能在 CDN 服务商自己的 AS 中高效传输，理论上最终送达用户所花的时间也会最小。 就好比我们开车在省内玩，肯定要比跨多个省经过多个收费站耗时要少。&lt;/p&gt;

&lt;h5 id=&quot;三-cdn-服务商所在自治系统-as-的相邻-as-越多离运营商骨干网越近数据传输也会更有优势&quot;&gt;三， CDN 服务商所在自治系统 AS 的相邻 AS 越多，离运营商骨干网越近，数据传输也会更有优势。&lt;/h5&gt;

&lt;p&gt;CDN 服务商所在的 AS 离运营商骨干网 AS 越近，理论上数据包传输所花时间也越少。另外， CDN 厂商如果同时租用了多个运营商品牌的带宽线路，其服务器的 IP 就会同时属于这几家运营商的 AS，跨运营商的数据传输时间也会比只有一个运营商的相对快些。就如同有多个高速公路的通行证，数据在传输过程中从一家的路面后就可直达用户，而不用来回在多个道路上切换，避免了不必要的时间损耗。&lt;/p&gt;

&lt;h2 id=&quot;哪家-cdn-功能最全&quot;&gt;哪家 CDN 功能最全&lt;/h2&gt;

&lt;p&gt;CDN 服务的功能点非常多，为了比较方便选择了 11 个常用的功能，主要覆盖加速优化，监控和安全三个方面：&lt;/p&gt;

&lt;h4 id=&quot;加速优化&quot;&gt;加速优化&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;HTTP2.0 加速&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HTTP2.0 和现在的 HTTP1.1 相比，做了很大的改动和优化，例如头部压缩、服务端推送等。因为它要求服务器端和浏览器端都得支持 HTTP2.0 协议，所以在国内获得普遍支持还有一段时间。不过作为互联网下一代 HTTP 协议，即使我们现在用不上，也应考虑为将来的系统升级留下余地。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;文件压缩&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前 CDN 节点使用的 WEB 服务器端普遍支持 GZIP 协议的压缩， 当用户浏览器访问静态资源，并且支持 Gzip 压缩时， 服务器端可以把资源压缩打包发送给浏览器，由浏览器进行解压， 减少文件在互联网传输的数据量和时间。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;源站推送&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了避免传统的 CDN 节点同时去源站拉数据，造成访问洪峰压垮源站的带宽和服务器。 CDN 厂商使用源站推送功能将源站内容提前推送给边缘 CDN 节点，提前进行刷新预热。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;点播加速&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CDN 对音视频等流媒体文件进行加速，其背后是一套复杂的技术方案，包括上传，转码，分发，以及 CDN 边缘节点根据用户终端支持协议的情况下发合适的流媒体格式。不同 CDN 服务商对点播加速的技术实现方案不同，不好做量化比较，只用是否支持播加速功能来比较。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;直播加速&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;直播加速如何解决播放延时、连麦时多路音视频的合并、以及突发热点对带宽的冲击等这些技术挑战，对 CDN 服务商的技术、硬件和网络条件都有很高的要求。也用是否支持直播加速来比较。&lt;/p&gt;

&lt;h4 id=&quot;监控统计&quot;&gt;监控统计&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;实时监控&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CDN 服务商提供图形化工具，对 CDN 的使用情况，例如点击量，命中率，公网下行流量等进行统计和监控。方便客户对于 CDN 使用效率和结果进行评估，及时发现问题和调整网络带宽预算。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;原始日志&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;提供所有客户终端访问 CDN 服务的原始日志 (access log)，这些日志看似没用，其实很有价值。例如，可以通过分析原始日志的数据包总量估算出 CDN 实际的下行流量，作为支付 CDN 服务费的参考。也可以通过分析这些日志的响应时间，结合客户端 IP，评估各地区终端用户实际访问 CDN 的情况&lt;/p&gt;

&lt;h4 id=&quot;安全&quot;&gt;安全&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;防盗链&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CDN 服务商防盗链的手段很多，例如常用的 http Referer 防盗链，其原理是利用 http header 中的 referer 属性，判断用户提交信息的网站 IP 地址，然后和真正的源站端的地址相比较，如果一致则表明是站内提交，或者为自己信任的站点提交，否则视为盗链。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;IP 黑白名单&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;黑白名单是我们可以根据业务需要对用户请求的源 IP 访问进行管理，为我们提供了主动防御的能力。使用 IP 黑名单的功能，可以有效的帮助我们阻止盗链，和恶意攻击。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SNI （ 服务器名称指示 Server Name Indication ）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;网站使用 SSL/TSL 协议校验是目前防止盗链，和解决 “DNS 劫持”最好的方式。目前国内外大型网站都已经换成了基于该协议的 HTTPS 通信方式。早期的 SSL 协议默认每个 IP 地址上只能用一个证书。TLSv1z 增加了服务器名称指示（SNI）功能，通过发送虚拟主机名作为 TLS 协商的一部分这使得服务器可以在握手阶段选择正确虚拟域，并发送对应证书。这样每个 IP 上可以部署多张证书，对于运行很多虚机和域名的用户会非常节省资源。如果 CDN 服务商支持该项功能，说明 CDN 服务支持 HTTPS 和 TSL v1 以上版本。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;OCSP 装订（OCSP STAPLING）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用 SSL 认证时，客户端会在 TLS 握手阶段，去发证机构对实时查询 OCSP （Online Certificate Status Protocol，在线证书状态协议）接口，来判断服务器端的证书是否作废。在获得获得 OCSP 结果前会阻塞后续流程，通常发证机构都在国外，客户端的访问会延迟整个 TLS 握手的时间。而 OCSP Stapling 功能，是指服务端在证书链中封装了发证书机构对证书的 OCSP 查询结果，从而让客户端浏览器跳过自己去验证的过程。如果 CDN 服务商支持该功能，说明其 CDN 服务支持 HTTPS 加速比较好。&lt;/p&gt;

&lt;h4 id=&quot;对比结果&quot;&gt;对比结果&lt;/h4&gt;

&lt;p&gt;对十家 CDN 服务商进行打分，结果如下表：
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/cdn_wx05.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;腾讯云和网宿科技得分最高，其它厂商各有优劣。 9 分以上的厂商占了前 50%，分别是腾讯云，网宿科技，阿里云，百度云蓝汛。后 50% 的厂商分数在 8-7 分之间。&lt;/p&gt;

&lt;p&gt;转载请注明出处，本文采用 &lt;a href=&quot;http://creativecommons.org/licenses/by-nc-nd/4.0/&quot;&gt;CC4.0&lt;/a&gt; 协议授权&lt;/p&gt;</content><author><name>Jevic</name></author><summary type="html">作为技术决策者在选择使用 CDN 服务时最关心的三个问题是：</summary></entry><entry><title type="html">分布式系统发展史</title><link href="http://0.0.0.0/2018/06/10/distributed-system/" rel="alternate" type="text/html" title="分布式系统发展史" /><published>2018-06-10T19:55:15+08:00</published><updated>2018-06-10T19:55:15+08:00</updated><id>http://0.0.0.0/2018/06/10/distributed-system</id><content type="html" xml:base="http://0.0.0.0/2018/06/10/distributed-system/">&lt;blockquote&gt;
  &lt;p&gt;分布式系统从最早的数据共享需求，发展到现在的 serverless 架构。它伴随着技术的发展与公司实际需求变化而演进。现在的云服务提供商简化了分布式系统开发的复杂性，让应用开发者只需关注开发，而把基础设施管理交给大型的云服务提供商。回顾分布式系统发展的历史，了解容器技术革新的原动力。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;分布式系统（确切地说应该是分布式计算机系统）从它诞生到现在已经过去了很长的时间。在很久以前，一台电脑一次只能完成一项特定的任务。如果我们需要同时完成多项任务，则需要多台计算机并行运行。但是，并行运行并不足以构建真正的分布式系统，因为它需要一种机制来在不同计算机或者那些运行在计算机上的程序之间进行通信。这种在多台计算机之间交换 / 共享数据的需求催生了面向消息通信的想法，即两台计算机使用包含了数据的消息来共享数据。文件共享、数据库共享等其他机制当时还没有出现。
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/DS_wxp01.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;接着，我们进入了多任务操作系统和个人电脑的时代。利用 Windows、Unix、Linux 等操作系统，我们可以在同一台计算机上运行多个任务。这使得分布式系统开发人员能够在一台或者几台通过消息传递连接的计算机内构建和运行整个分布式系统。这催生了面向服务的架构（SOA），其中每个分布式系统可以通过一组集成在一台计算机或多台计算机上运行的服务来构建。我们通过 WSDL（用于 SOAP 协议）或 WADL（用于 REST 协议）等语言适当地定义服务接口。接着，服务的使用者将利用这些接口来进行客户端的实现。
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/DS_wxp02.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;随着计算能力和存储价格的降低，世界各地的组织都开始使用分布式系统和基于 SOA 的企业 IT 系统。但是，一旦服务或系统的数量增加，这些服务之间的点到点连接就不再是可扩展和可维护的了。这催生了集中式“服务总线”概念的产生。服务总线通过类似集线器的架构将所有系统连接在一起。这个组件被称为 ESB（企业服务总线）。它作为一个“语言”翻译者，就像一个中间人在帮助一群使用不同“语言”但希望相互通信的人进行沟通。在企业应用中，“语言”代表着在通信时不同系统的消息传递协议和消息格式。
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/DS_wxp03.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这种模式工作得很好，即使在今天也能正常工作。随着万维网的普及和模型的简化，基于 REST 的通信比基于 SOAP 的通信模型变得更加流行。这促进了基于应用程序编程接口（API）的 REST 模型通信的发展。由于 REST 模型的简洁特性，我们需要在标准 REST API 实现之上实现安全（身份验证和授权）、缓存、流控和监控等各种类型的功能。但我们并不想独立地在每个 API 上实现这些功能，而是需要一个公共组件将这些功能应用于这些 API 之上。这样的需求催生了 API 管理平台的发展。现在，它已经成为了任何分布式系统的核心功能之一。
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/DS_wxp04.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;随后，我们见证了分布式系统大爆炸的时代。Facebook、Google、Amazon、Netflix、LinkedIn、Twitter 等互联网公司变得异常庞大。他们开始想要构建跨越多个地理区域和多个数据中心的分布式系统。这样的需求使他们的技术焦点转向了一切开始的地方。工程师们开始思考单台计算机和单个程序的概念。他们不再把一台计算机当作一台计算机来看，而在同一台计算机内创建多台虚拟计算机。这催生了关于虚拟机的想法，即同一台计算机可以充当多台计算机并且全部并行运行。尽管这是一个还不错的主意，但在宿主计算机的资源利用方面，这并不是最好的选择。运行多个操作系统需要更多的资源，但在同一个操作系统里运行多个程序并不需要这些资源。&lt;/p&gt;

&lt;p&gt;这些问题最终催生了关于容器技术的想法。容器只使用一个宿主操作系统（Linux）的内核，就可以运行多个程序并分别依赖于相互独立的运行时。这个概念在 Linux 操作系统上已经有一段时间了。随着基于容器技术的应用程序部署的普及，它变得更加流行并且有了很多改进和提升。容器可以像虚拟机一样工作，却不需要多一个操作系统的开销。您可以将应用程序和所有相关的依赖项放入容器镜像中。它便可以被放在任何可以运行容器的宿主操作系统中运行。Docker 和 Rocket 是两个热门的容器构建平台。
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/DS_wxp05.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;容器技术为 Netflix、LinkedIn 和 Twitter 等组织提供了底层框架，用于构建他们要求苛刻的永远在线的多区域、多数据中心应用平台。但这并不意味着利用容器技术没有任何难点。基于容器的部署带来的轻量特性让跨多个容器的平台维护和编排变得非常复杂。随着微服务架构（MSA）的出现，单体式应用程序被分成更小块的微服务。这些微服务能够完成整个服务里的某一个特定功能并部署在容器中（在大多数情况下都可以）。这给分布式系统生态系统带来了一系列新的需求。要让系统最终保持一致，并且彼此之间没有太多复杂的通信。
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/DS_wxp06.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这些新的需求最终帮助工程师们构建了一个容器编排系统。该系统可用于维护更大规模的容器部署的一致性。毋庸置疑的是，这个领域的顶尖技术来自 Google。因为它们的规模非常大。他们构建了名为“Kubernetes”（又名 k8s）的容器编排平台，并成为大规模容器编排需求的事实标准。k8s 让工程师可以：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在大型集群中运行容器&lt;/li&gt;
  &lt;li&gt;将数据中心视为一台计算机&lt;/li&gt;
  &lt;li&gt;控制服务之间的通信（在容器上运行）&lt;/li&gt;
  &lt;li&gt;动态伸缩与为多个服务进行负载均衡&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes 和 Docker 让应用程序员的生活更加轻松。他们不用再考虑他们的应用在不同的环境（操作系统、开发环境、测试环境、生产环境等）下的不同表现。他构建的容器镜像在所有环境中运行表现几乎完全相同，因为所有依赖项都被打包到镜像中了。
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/DS_wxp07.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;但是，尽管我们有了容器和编排框架，我们仍然需要一个管理这些服务器的团队。这意味着数据中心需要使用像 Docker 和 Kubernetes 这样的技术进行管理，以确保它对于应用程序来说就像一个单台计算机一样。如果不是你自己来做这些事情，而是别人来为你管理这部分工作，这正是 serverless 架构所带来的便利。您的服务器将由第三方云提供商（如 Amazon（Lambda），Microsoft（Azure Functions）或 Google（Cloud Functions））进行管理。现在，分布式系统将由应用程序员进行编程，而基础设施管理将由云提供商完成。这是分布式系统发展的最新状态，并且会不断地发展下去。&lt;/p&gt;

&lt;p&gt;转载请注明出处，本文采用 &lt;a href=&quot;http://creativecommons.org/licenses/by-nc-nd/4.0/&quot;&gt;CC4.0&lt;/a&gt; 协议授权&lt;/p&gt;</content><author><name>Jevic</name></author><summary type="html">分布式系统从最早的数据共享需求，发展到现在的 serverless 架构。它伴随着技术的发展与公司实际需求变化而演进。现在的云服务提供商简化了分布式系统开发的复杂性，让应用开发者只需关注开发，而把基础设施管理交给大型的云服务提供商。回顾分布式系统发展的历史，了解容器技术革新的原动力。</summary></entry><entry><title type="html">kafka 数据保存时间动态调整</title><link href="http://0.0.0.0/2018/06/07/kafka-data-clear/" rel="alternate" type="text/html" title="kafka 数据保存时间动态调整" /><published>2018-06-07T22:07:44+08:00</published><updated>2018-06-07T22:07:44+08:00</updated><id>http://0.0.0.0/2018/06/07/kafka-data-clear</id><content type="html" xml:base="http://0.0.0.0/2018/06/07/kafka-data-clear/">&lt;blockquote&gt;
  &lt;p&gt;动态调整kafka 数据保存时间,清理过期数据!!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## kafka 版本: kafka_2.10-0.8.2.2&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;topics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;test1 test2&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;zk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;zk1:2181,zk2:2181,zk3:2181/kafka&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### 修改保留时间&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### 保留几个小时&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;hours&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2
&lt;span class=&quot;c&quot;&gt;### 转换为毫秒&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;Times&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$hours&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; * 3600000&quot;&lt;/span&gt;|bc&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## 修改配置&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### 控制未压缩数据 retention.ms&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;retention.ms&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### 控制压缩后的数据 delete.retention.ms&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#conf=&quot;delete.retention.ms&quot;&lt;/span&gt;

ClearLog&lt;span class=&quot;o&quot;&gt;(){&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## 清理数据&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;i &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$topics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;do
&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;/opt/kafka/bin/kafka-topics.sh --zookeeper &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$zk&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; --alter --topic &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$i&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; --config &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Times&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#/opt/kafka/bin/kafka-topics.sh --zookeeper $zk --alter --topic $i --config ${conf}=${Times}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

DelConf&lt;span class=&quot;o&quot;&gt;(){&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## 删除配置&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;/opt/kafka/bin/kafka-topics.sh --zookeeper &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$zk&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; --alter --topic &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$i&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; --delete-config &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$conf&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#/opt/kafka/bin/kafka-topics.sh --zookeeper $zk --alter --topic $i --delete-config $conf&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in
     &lt;/span&gt;clear&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
     ClearLog &lt;span class=&quot;p&quot;&gt;;;&lt;/span&gt;
     delconf&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
     DelConf &lt;span class=&quot;p&quot;&gt;;;&lt;/span&gt;
     &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
     &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;clear --- 清理日志&quot;&lt;/span&gt;
     &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;delconf --- 删除配置&quot;&lt;/span&gt;
     &lt;span class=&quot;p&quot;&gt;;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;esac&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;转载请注明出处，本文采用 &lt;a href=&quot;http://creativecommons.org/licenses/by-nc-nd/4.0/&quot;&gt;CC4.0&lt;/a&gt; 协议授权&lt;/p&gt;</content><author><name>Jevic</name></author><summary type="html">动态调整kafka 数据保存时间,清理过期数据!!</summary></entry><entry><title type="html">mirrors 国内镜像站</title><link href="http://0.0.0.0/2018/05/25/mirror/" rel="alternate" type="text/html" title="mirrors 国内镜像站" /><published>2018-05-25T16:42:16+08:00</published><updated>2018-05-25T16:42:16+08:00</updated><id>http://0.0.0.0/2018/05/25/mirror</id><content type="html" xml:base="http://0.0.0.0/2018/05/25/mirror/">&lt;h2 id=&quot;开源镜像站点&quot;&gt;开源镜像站点&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mirrors.163.com/&quot;&gt;网易开源镜像站&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://opsx.alibaba.com/mirror&quot;&gt;阿里云开源镜像站&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mirror.azure.cn/&quot;&gt;开源社区镜像站&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mirror.tuna.tsinghua.edu.cn/&quot;&gt;清华大学开源镜像站&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;docker-镜像加速器&quot;&gt;docker 镜像加速器&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;daocloud: https://www.daocloud.io/mirror&lt;/li&gt;
  &lt;li&gt;阿里云: https://yq.aliyun.com/articles/29941&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;gcrgoogle-container-registry镜像&quot;&gt;GCR（Google Container Registry）镜像&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;开源社区镜像: https://gcr.akscn.io/google_containers​&lt;/li&gt;
  &lt;li&gt;阿里云镜像: https://dev.aliyun.com/search.html&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;阿里云镜像比较废材,版本都没有更新,所以这里使用开源社区镜像提供的即可。
在部署kubernetes时,通常所有用到的镜像都可以在&lt;code class=&quot;highlighter-rouge&quot;&gt;gcr.akscn.io/google_containers&lt;/code&gt; 项目中 直接pull到本地
修改标签即可或者直接修改yaml 文件中指定的镜像名称为本地镜像名称以便使用。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;使用示例:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull gcr.akscn.io/google_containers/kubernetes-dashboard-amd64:v1.10.0
docker pull gcr.akscn.io/google_containers/pause-amd64:3.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pull&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# docker images|grep google
gcr.akscn.io/google_containers/heapster-amd64                          v1.5.4              72d68eecf40c        4 months ago        75.3MB
gcr.akscn.io/google_containers/heapster-influxdb-amd64                 v1.5.2              eb180058aee0        4 months ago        16.5MB
gcr.akscn.io/google_containers/heapster-grafana-amd64                  v5.0.4              25e1da333f76        4 months ago        171MB
gcr.akscn.io/google_containers/coredns                                 1.0.6               d4b7466213fe        9 months ago        39.9MB
gcr.akscn.io/google_containers/kubernetes-dashboard-amd64              v1.8.3              0c60bcf89900        9 months ago        102MB
gcr.akscn.io/google_containers/cluster-proportional-autoscaler-amd64   1.1.2-r2            7d892ca550df        17 months ago       49.6MB
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;kubernetes-rpmdeb镜像&quot;&gt;Kubernetes RPM/DEB镜像&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;阿里云镜像&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示例:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# CentOS
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
​
# Ubuntu
cat &amp;lt;&amp;lt;EOF &amp;gt;/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;helm-charts-镜像&quot;&gt;Helm Charts 镜像&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Helm: http://mirror.azure.cn/kubernetes/helm/​&lt;/li&gt;
  &lt;li&gt;Stable Charts: http://mirror.azure.cn/kubernetes/charts/​&lt;/li&gt;
  &lt;li&gt;Incubator Charts: http://mirror.azure.cn/kubernetes/charts-incubator/​&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;helm repo add stable http://mirror.azure.cn/kubernetes/charts/
helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Jevic</name></author><summary type="html">开源镜像站点 网易开源镜像站 阿里云开源镜像站 开源社区镜像站 清华大学开源镜像站</summary></entry><entry><title type="html">docker 问题定位记述</title><link href="http://0.0.0.0/2018/05/06/docker-problem-location/" rel="alternate" type="text/html" title="docker 问题定位记述" /><published>2018-05-06T17:56:26+08:00</published><updated>2018-05-06T17:56:26+08:00</updated><id>http://0.0.0.0/2018/05/06/docker-problem-location</id><content type="html" xml:base="http://0.0.0.0/2018/05/06/docker-problem-location/">&lt;blockquote&gt;
  &lt;p&gt;性能测试发现业务进程运行在容器中比业务进程运行在宿主机上吞吐量下降了 100 倍，这让周一显得更加阴暗。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;周一&quot;&gt;周一&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;先找下游了解了下业务模型，他们说已经把业务模型最简化了，当前的模式是：业务进程运行在容器中，通过与主机共享 IPC namespace 的方式来使用共享内存与宿主机上的 Daemon 进程进行通信，整个过程不涉及磁盘读写、网络交互等。
撸起袖子开始干，定位第一步，当然是找瓶颈了，分析下到底问题出在哪。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;top&quot;&gt;top&lt;/h5&gt;

&lt;p&gt;用到的第一个命令自然是 top，top 是 linux 里一个非常强大的命令，通过它基本上能看到系统中的所有指标。
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/docker-error-wx01.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上面是 top 命令运行时的一个示意图，比较重要的指标都已经标了出来：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;1 处表示系统负载，它表示当前正在等待被 cpu 调度的进程数量，这个值小于系统 vcpu 数（超线程数）的时候是比较正常的，一旦大于 vcpu 数，则说明并发运行的进程太多了，有进程迟迟得不到 cpu 时间。这种情况给用户的直观感受就是敲任何命令都卡。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;2 处表示当前系统的总进程数，通常该值过大的时候就会导致 load average 过大。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;3 处表示 cpu 的空闲时间，可以反应 cpu 的繁忙程度，该值较高时表示系统 cpu 处于比较清闲的状态，如果该值较低，则说明系统的 cpu 比较繁忙。需要注意的是，有些时候该值比较高，表示 cpu 比较清闲，但是 load average 依然比较高，这种情况很可能就是因为进程数太多，进程切换占用了大量的 cpu 时间，从而挤占了业务运行需要使用的 cpu 时间。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;4 处表示进程 IO 等待的时间，该值较高时表示系统的瓶颈可能出现在磁盘和网络。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;5 处表示系统的剩余内存，反应了系统的内存使用情况。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;6 处表示单个进程的 cpu 和内存使用情况。关于 top 命令中各个指标含义的进一步描述可以参见：&lt;/li&gt;
  &lt;li&gt;http://www.jb51.net/LINUXjishu/34604.html&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用 top 命令查看了下系统情况，发现一切正常。load average 也不高，task 也不多，cpu 和内存都还很空闲，就连 IO 等待时间都很低，也没有哪个进程的 cpu 和内存使用率偏高，一切都很和谐，没有瓶颈！&lt;/p&gt;

&lt;p&gt;当然，没有瓶颈是不可能的。由于我们的容器都是绑核的，所以很有可能是分配给容器的那些核是处于繁忙状态，而由于总核数较多，将 cpu 的使用率给拉了下来。于是又按下了“1”键，切换到详细模式下：
&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/docker-error-wx02.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这种模式下，可以看到每个 vcpu 的使用情况。一切依然很和谐，诡异的和谐。
看来从 cpu 这块是看不出来什么了，那就继续看看是不是磁盘搞的鬼吧。&lt;/p&gt;

&lt;h5 id=&quot;iostate&quot;&gt;iostate&lt;/h5&gt;
&lt;blockquote&gt;
  &lt;p&gt;iostate 命令是用来查看磁盘使用情况的一个命令，经验告诉我们，磁盘和网络已经成为影响性能的最大嫌疑犯。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;使用 iostate 工具时，通常只用关注最后一行（%util）即可，它反映了磁盘的繁忙程度。虽然下游部门已经说了他们跑的用例是一个纯内存的场景，不涉及磁盘读写。但是客户的话信得住，母猪也能上树，我还是要跑一下 iostate，看下磁盘情况怎么样。结果，依旧很和谐，磁盘使用率基本为零。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;后面还尝试了观察网络指标，发现确实也没有网络吞吐，完了，看来问题没那么简单。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;周二&quot;&gt;周二&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;虽然周一看似白忙活了一天，但是也得到了一个重要结论：这个问题不简单！不过简单的在资源层面时分析不出来啥了，得寄出性能分析的大杀器——perf 了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;perf-火焰图&quot;&gt;perf+ 火焰图&lt;/h5&gt;
&lt;blockquote&gt;
  &lt;p&gt;perf 是 linux 下一个非常强大的性能分析工具，通过它可以分析出进程运行过程中的主要时间都花在了哪些地方。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;之前没太使用过 perf，因此刚开始进行分析，就自然而然地直接使用上了 perf+ 火焰图这种最常见的组合：&lt;/p&gt;

&lt;p&gt;安装 perf。&lt;/p&gt;

&lt;p&gt;yum install perf&lt;/p&gt;

&lt;p&gt;下载火焰图工具。&lt;/p&gt;

&lt;p&gt;git clone https://github.com/brendangregg/FlameGraph.git&lt;/p&gt;

&lt;p&gt;采样。&lt;/p&gt;

&lt;p&gt;perf record -e cpu-clock -g -p 1572（业务进程 id）&lt;/p&gt;

&lt;p&gt;一段时间（通常 20s 足够）之后 ctrl+c，结束采样。&lt;/p&gt;

&lt;p&gt;用 perf script 工具对 perf.data 进行解析。&lt;/p&gt;

&lt;p&gt;perf script -i perf.data &amp;amp;&amp;gt; perf.unfold。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PS：如果在容器中运行的程序有较多的依赖，则该命令解析出来的符号中可能会有较多的“Unregistered symbol…”错误，此时需要通过–symfs参数指定容器的rootfs位置来解决该问题。获取容器rootfs的方法根据 docker 的 storagedriver 的不同而有所不同，如果是device mapper类型，则可以通过 dockerinspect 找到容器的rootfs所在位置，如果是overlay类型，则需要通过 dockerexport 命令将该容器的rootfs导出来，如果是富容器的话，一般都有外置的rootfs，直接使用即可。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将 perf.unfold 中的符号进行折叠。&lt;/p&gt;

&lt;p&gt;./stackcollapse-perf.pl perf.unfold &amp;amp;&amp;gt; perf.folded&lt;/p&gt;

&lt;p&gt;最后生成 svg 图。&lt;/p&gt;

&lt;p&gt;/flamegraph.pl perf.folded &amp;gt; perf.svg&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jevic/images/master/docker/docker-error-wx03.png&quot; alt=&quot;&quot; /&gt;
最后就能得到像下面这种样子的漂亮图片。通常情况下，如果程序中有一些函数占用了大量的 CPU 时间，则会在图片中以长横条的样式出现，表示该函数占用了大量的 CPU 时间。&lt;/p&gt;

&lt;p&gt;然而，perf+ 火焰图在这次并没有起到太大的作用，反复统计了很多次，并没有出现梦寐以求的“长横条”，还是很和谐。&lt;/p&gt;

&lt;h5 id=&quot;perf-stat&quot;&gt;perf stat&lt;/h5&gt;
&lt;p&gt;perf+ 火焰图并没有起到很好的效果，就想换个工具继续试试，但是找来找去、请教大神，也没找到更好的工具，只好继续研究 perf 这个工具。&lt;/p&gt;

&lt;p&gt;perf 除了上面提到的 record（记录事件）、script（解析记录的事件）命令之外，还有其他一些命令，常用的有 report（与 script 类似，都是对 perf record 记录的事件进行解析，不同之处在于 report 直接解析程序中的运行热点，script 的扩展性更强一点，可以调用外部脚本对事件数据进行解析）、stat（记录进程一段时间之内触发的事件数）、top（实时分析程序运行时热点）、list（列出 perf 可以记录的事件数）等命令。&lt;/p&gt;

&lt;p&gt;这些命令挨个试了个遍，终于在 perf stat 命令这块有了突破：&lt;/p&gt;

&lt;p&gt;使用 perf stat 对业务进程运行在物理机和容器上分别进行统计，发现业务进程运行在容器中时，大部分事件（task-clock、context-switches、cycles、instructions 等）的触发次数是运行在物理机上时的百分之一。&lt;/p&gt;

&lt;p&gt;这是什么原因呢？一定是什么东西阻塞住了程序的运转，这个东西是什么呢？&lt;/p&gt;

&lt;p&gt;前面已经分析了，不是磁盘，不是网络，也不是内存，更不是 cpu，那还有什么呢？？&lt;/p&gt;

&lt;h4 id=&quot;周三&quot;&gt;周三&lt;/h4&gt;

&lt;p&gt;是什么原因阻塞住了程序的运转呢？百思不得其解，百问不得其解，百猜不得其解，得，还是得动手，上控制变量法。&lt;/p&gt;

&lt;p&gt;运行在容器中的程序和运行在物理机上有什么区别呢？我们知道，docker 容器 =cgroup+namespace+secomp+capability+selinux，那么就把这些技术一个个都去掉，看到底是哪个特性搞的鬼。&lt;/p&gt;

&lt;p&gt;在这 5 个技术中，后三个都是安全相关的，都有开关可以控制，经过测试，发现把后三个都关掉之后，性能还是很差，说明这 3 个技术是无辜的，开始排查 cgroup 和 namespace。&lt;/p&gt;

&lt;p&gt;首先怀疑的当然是 cgroup，毕竟它就是做资源限制的，很有可能一不小心限制错了，就把业务给限制了。&lt;/p&gt;

&lt;h5 id=&quot;cgexec&quot;&gt;cgexec&lt;/h5&gt;
&lt;blockquote&gt;
  &lt;p&gt;cgexec 是 cgroup 提供的一个工具，可以在启动时就将程序运行到某个 cgroup 中，因此我们可以将业务程序运行在物理机上，但是放到业务容器所在的 cgroup 中，看看性能会不会下降。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;具体用法如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cgexec -g *:/system.slice/docker-03c2dd57ba123879abab6f7b6da5192a127840534990c515be325450b7193c11.scope ./run.sh

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;通过该命令即可将 run.sh 运行在与容器 03c2dd57 相同的 cgroup 中。在多次测试之后，发现这种情况下，业务进程的运行速度没有受到影响，cgroup 被洗白，那么真相只有一个——凶手就是 namespace。&lt;/p&gt;

&lt;h4 id=&quot;周四&quot;&gt;周四&lt;/h4&gt;

&lt;p&gt;虽然说凶手已经确定是 namespace，但是 namespace 家族也有一大票人，有 ipc namespace、pid namespace 等等，还需要进一步确定真凶。&lt;/p&gt;

&lt;h5 id=&quot;nsenter&quot;&gt;nsenter&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;nsenter 是一个 namespace 相关的工具，通过它可以进入某个进程所在的 namespace。在 docker exec 命令出现之前，它唯一一个可以进入 docker 容器的工具，在 docker exec 出现之后，nsenter 也由于其可以选择进入哪些 namespace 而成为 docker 问题定位的一个极其重要的工具。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过如下命令，即可进入容器所在的 mount namespace。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nsenter --target $(docker inspect --format '' 容器 id) --mount bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;同理，通过如下命令即可进入容器所在的 IPC namespace 和 pid namespace。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nsenter --target $(docker inspect --format '' 容器 id) --ipc --pid bash

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在不断的将业务进程在各个 namespace 之间切换后，终于进一步锁定了真凶：mount namespace。测试发现，一旦将业务进程放置到容器所在的 mount namespace，性能就会急剧下降。&lt;/p&gt;

&lt;p&gt;这是为什么呢？这是为什么呢？mount namespace 到底做了什么，会有这么大的影响？&lt;/p&gt;

&lt;h4 id=&quot;周五&quot;&gt;周五&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mount namespace&lt;/code&gt; 为什么有这么大的威力呢？它到底影响什么了呢？实在想不通，就去请教了下大神，大神想了想，回了我句，试试 ldd？&lt;/p&gt;

&lt;h5 id=&quot;ldd&quot;&gt;ldd&lt;/h5&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ldd 是什么？&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;当然这句话我没去问大神，转身自己查去了。&lt;/p&gt;

&lt;p&gt;ldd 是 list, dynamic, dependencies 的缩写，意思是列出动态库依赖关系。顿时豁然开朗，mount namespace 隔离出了自己的文件系统，所以容器内外可以使用不同的依赖库，而不同的依赖库就可能造成无数种影响。&lt;/p&gt;

&lt;p&gt;于是开始通过 ldd 对比容器中业务进程的依赖库与宿主机上业务进程的依赖库，最终发现容器中的 glibc 库与宿主机上的 glibc 库版本不一致，很可能是这个原因导致性能下降的。&lt;/p&gt;

&lt;p&gt;于是将容器中的 glibc 库版本替换为宿主机上的 glibc 库之后，容器内业务的性能终于恢复了，猜想得到证实。&lt;/p&gt;

&lt;h4 id=&quot;后记&quot;&gt;后记&lt;/h4&gt;

&lt;p&gt;为什么容器内外 glibc 版本不一致就导致性能下降了呢？&lt;/p&gt;

&lt;p&gt;这是和业务模型相关的，前面提到，下游的业务模型是通过与主机共享 IPC namespace 的方式来使用共享内存与宿主机上的 daemon 进程进行通信。而 glibc 在一次升级中，更新了信号量的数据结构（如下），就会导致在共享内存通信时，由于数据格式不一致，每次信号量通信都超时，从而影响了程序运行效率。&lt;/p&gt;

&lt;p&gt;转载请注明出处，本文采用 &lt;a href=&quot;http://creativecommons.org/licenses/by-nc-nd/4.0/&quot;&gt;CC4.0&lt;/a&gt; 协议授权&lt;/p&gt;</content><author><name>Jevic</name></author><summary type="html">性能测试发现业务进程运行在容器中比业务进程运行在宿主机上吞吐量下降了 100 倍，这让周一显得更加阴暗。</summary></entry><entry><title type="html">12款Kubernetes发行版</title><link href="http://0.0.0.0/2018/04/27/k8s-release-12/" rel="alternate" type="text/html" title="12款Kubernetes发行版" /><published>2018-04-27T20:35:46+08:00</published><updated>2018-04-27T20:35:46+08:00</updated><id>http://0.0.0.0/2018/04/27/k8s-release-12</id><content type="html" xml:base="http://0.0.0.0/2018/04/27/k8s-release-12/">&lt;blockquote&gt;
  &lt;p&gt;12 款最突出的 Kubernetes 产品，也就是整合了 Kubernetes 和容器工具的发行版。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://d33wubrfki0l68.cloudfront.net/1567471e7c58dc9b7d9c65dcd54e60cbf5870daa/a2249/images/flower.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes 已经成为大规模容器编排的首选。这款由 Google 开源的容器编排系统受到广泛好评和支持，发展势头迅猛。&lt;/p&gt;

&lt;p&gt;Kubernetes 十分庞大而复杂，难以搭建和配置。不仅如此，还有很多繁重的工作留给了最终用户去做。因此，最好的方法就是不要尝试单独使用 Kubernetes，而是寻找一种将 Kubernetes 作为组件的容器解决方案。&lt;/p&gt;

&lt;p&gt;在这里，我列出了 12 款最突出的 Kubernetes 产品，也就是整合了 Kubernetes 和容器工具的发行版，可媲美各种供应商提供的 Linux 内核版本。&lt;/p&gt;

&lt;p&gt;请注意，此列表不包含专有云服务，如 Amazon EKS 或 Google Kubernetes Engine，我们主要关注的是可在本地运行或作为云托管服务的产品。&lt;/p&gt;

&lt;h4 id=&quot;coreos-tectonic&quot;&gt;CoreOS Tectonic&lt;/h4&gt;
&lt;p&gt;CoreOS 主要提供基于容器的 Linux 发行版，与 Docker 兼容，但具有自己的自定义镜像格式和运行时，以及“企业级 Kubernetes”发行版。它们一起构成了 CoreOS Tectonic 技术栈的基础。&lt;/p&gt;

&lt;p&gt;CoreOS 的操作系统 Container Linux 主要作为一组容器化的组件进行发行。通过这种方式，操作系统的自动更新可以直接进入生产环境中，无需关闭运行中的应用程序。CoreOS 也支持“一键”更新 Kubernetes。CoreOS Tectonic 可运行在 Amazon Web Services、Microsoft Azure 和裸机上。&lt;/p&gt;

&lt;h4 id=&quot;kubernetes-的-canonical-发行版&quot;&gt;Kubernetes 的 Canonical 发行版&lt;/h4&gt;

&lt;p&gt;Canonical 是 Ubuntu Linux 的制造商，也提供了自己的 Kubernetes 发行版。Canonical 发行版的一大卖点是它以已经得到广泛推崇和部署的 Ubuntu Linux 发行版为基础。 Canonical 称它的技术栈可以在云端或本地运行，并且支持 CPU 和 GPU 两种工作负载。对于付费用户，有 Canonical 工程师来远程协助管理 Kubernetes 群集。&lt;/p&gt;

&lt;p&gt;Canonical 和 Rancher 实验室（见下文）共同推出了 Cloud Native Platform，将 Canonical 的 Kubernetes 发行版与 Rancher 的容器管理平台组合在一起。该平台使用 Kubernetes 来管理每个集群中运行的容器，并使用 Rancher 来管理多个 Kubernetes 集群。Cloud Native Platform 将随 Rancher 2.0 一起发布，目前提供了测试预览版。&lt;/p&gt;

&lt;h4 id=&quot;docker-社区版和-docker-企业版&quot;&gt;Docker 社区版和 Docker 企业版&lt;/h4&gt;

&lt;p&gt;对于大多数人来说，Docker 就是容器。自 2014 年起，Docker 拥有了自己的集群和编排系统 Docker Swarm，不久前 Docker Swarm 还是 Kubernetes 的竞争对手。2017 年 10 月，Docker 宣布将 Kubernetes 作为 Docker 社区版和 Docker Enterprise 2.0 的标准插入式组件。&lt;/p&gt;

&lt;p&gt;简而言之，Docker 公司承认 Kubernetes 比 Swarm 更适合管理大型复杂的容器环境。但是，Docker 仍然为小型作业保留了初始的集群系统（也就是“Swarm 模式”），比如部署在数量不会很大的防火墙后面的本地应用程序。&lt;/p&gt;

&lt;h4 id=&quot;heptio-kubernetes-subscription&quot;&gt;Heptio Kubernetes Subscription&lt;/h4&gt;
&lt;p&gt;Kubernetes 的两位作者 Craig McLuckie 和 Joe Beda 共同创立了 Heptio，旨在提供基于 Kubernetes 的服务和产品。他们的第一个主要产品是 Heptio Kubernetes Subscription（HKS），一项付费的 Kubernetes 部署服务，由 Heptio 提供 24/7 全天候支持。起价为每月 2000 美元。&lt;/p&gt;

&lt;p&gt;Heptio 的主要卖点是提供没有供应商锁定的企业级 Kubernetes。该产品可以运行在公有云或私有硬件上。Heptio 提供的所有用于管理和配置 Kubernetes 的工具都是开源的，补丁可以直接推送到受支持的集群。&lt;/p&gt;

&lt;h4 id=&quot;mesosphere-dcos&quot;&gt;Mesosphere DC/OS&lt;/h4&gt;

&lt;p&gt;Mesosphere DC/OS 使用 Apache Mesos 将一组机器变成单个资源，并可以动态分配给多个应用程序。 Kubernetes 作为 DC/OS 上众多应用程序包之一，用户可以跨 DC/OS 群集安装、运行和更新 Kubernetes。&lt;/p&gt;

&lt;p&gt;Kubernetes 并不完全是 DC/OS 的一部分，但可以通过 DC/OS 来部署，就像 Linux 应用程序可以通过 Linux 发行版的包管理系统来管理一样，这么说来，DC/OS 本身是不是一个 Kubernetes 发行版仍然值得商榷。尽管如此，Mesosphere 在如何使用 Kubernetes 方面与 Kubernetes 的工作原理息息相关，例如，它使用 Kubernetes 的主流社区发行版来确保与现有工具集的高度兼容性。&lt;/p&gt;

&lt;h4 id=&quot;mirantis-cloud-platform&quot;&gt;Mirantis Cloud Platform&lt;/h4&gt;

&lt;p&gt;如 Mirantis 所言，Mirantis Cloud Platform 将 OpenStack、Kubernetes 或两者的组合作为“敏捷基础设施平台”的基础。简而言之，Mirantis Cloud Platform 是一个用于编排虚拟机、容器和裸机服务器的单一集成解决方案。该平台以“DevOps 方式”管理部署在该平台上的应用程序，使用 Salt 作为配置管理工具，并集成 CI/CD 支持以确保应用程序被正确部署。&lt;/p&gt;

&lt;p&gt;Mirantis Cloud Platform 可以直接在裸机、OpenStack 集群或公有云上运行 Kubernetes。&lt;/p&gt;

&lt;p&gt;Mirantis 声称，Mirantis Cloud Platform 可以更容易地与 Kubernetes 集成，因为配置 Kubernetes 基础设施的相关任务不会落在最终用户身上。&lt;/p&gt;

&lt;h4 id=&quot;platform9-managed-kubernetes&quot;&gt;Platform9 Managed Kubernetes&lt;/h4&gt;

&lt;p&gt;大多数 Kubernetes 发行版专注于让 Kubernetes 从内到外和从上到下都易于管理。 Platform9 Managed Kubernetes 可以在任意环境中运行——本地裸机或远程的公共有云上，并可由 Platform9 的工程师作为服务进行远程管理。&lt;/p&gt;

&lt;p&gt;在客户的监督下，Platform9 大约每六周推出一次 Managed Kubernetes 更新。 Platform9 还提供了一些功能，比如多租户用户配额，而该功能在 Kubernetes 集群中通常需要通过手动来添加。Platform9 还提供了与 Platform9 Fission 项目的集成，Fission 是一个无服务器计算服务（“函数即服务”系统），可与大多数具有容器化运行时的编程语言一起使用。&lt;/p&gt;

&lt;h4 id=&quot;rancher-20&quot;&gt;Rancher 2.0&lt;/h4&gt;

&lt;p&gt;Rancher 实验室已经将 Kubernetes 集成到它的容器管理平台 Rancher 2.0 版本中，Rancher 2.0 目前处于测试阶段。相比其他 Kubernetes 发行版，Rancher 2.0 位于更上层，它位于 Linux 主机、Docker 容器和 Kubernetes 节点之上，可以独立管理所有这些节点。它甚至可以管理 Amazon EKS、Google Kubernetes Engine、Azure Container Service 和其他云端的 Kubernetes。&lt;/p&gt;

&lt;p&gt;Rancher 也有自己的 Kubernetes 发行版。Rancher 旨在消除搭建 Kubernetes 集群和为特定环境定制 Kubernetes 所需要的苦差事，并防止这些自定义功能妨碍 Kubernetes 升级。&lt;/p&gt;

&lt;h4 id=&quot;red-hat-openshift&quot;&gt;Red Hat OpenShift&lt;/h4&gt;

&lt;p&gt;Red Hat 的 PaaS 产品 OpenShift 最初使用 Heroku 风格的“cartridges”来打包应用程序，然后把它们部署到名为“gear”的容器中。后来，Docker 出现了，OpenShift 进行了重写，以便利用新的容器镜像和运行时标准。Red Hat 也不可避免地将 Kubernetes 作为 OpenShift 的编配技术。&lt;/p&gt;

&lt;p&gt;OpenShift 旨在为 PaaS 中的所有组件提供抽象和自动化。这种抽象和自动化也扩展到了 Kubernetes，因此带来了相当大的管理负担，而 OpenShift 可以用来在部署 PaaS 的过程中缓解这一点。&lt;/p&gt;

&lt;h4 id=&quot;stackube&quot;&gt;Stackube&lt;/h4&gt;

&lt;p&gt;Hyper.sh 云服务用于运行容器，它的开发商 HyperHQ 推出了 Stackube，一个“以 Kubernetes 为中心的 OpenStack 发行版”。通常，OpenStack 使用一个名为 Nova 的组件来配置和管理计算节点，而 Stackube 使用的是 Kubernetes。除此之外，它使用的是“普通”的 OpenStack 和 Kubernetes，所有其他额外细节由 OpenStack 插件来处理。&lt;/p&gt;

&lt;p&gt;HyperHQ 声称，Stackube 的主要优势是它可以根据使用哪个容器运行时提供不同程度的多租户。对于“软”多租户，可以使用 Docker，要想更可靠地进行资源分离，可以使用 HyperContainer，HyperContainer 提供了 Hypervisor 级别的隔离。&lt;/p&gt;

&lt;h4 id=&quot;suse-caas-平台&quot;&gt;SUSE CaaS 平台&lt;/h4&gt;

&lt;p&gt;SUSE 以在欧洲广泛流行的 Linux 发行版而闻名，它还提供了 SUSE CaaS 平台。从概念上讲，它让人联想到 CoreOS Tectonic——捆绑运行容器的裸机“微”操作系统，将 Kubernetes 作为容器编排系统，内置镜像注册表和集群配置工具。&lt;/p&gt;

&lt;p&gt;SUSE CaaS Platform 可以在公有云以及本地裸机上运行，但要注意，“SUSE 目前不支持任何与底层云基础设施的集成”。这意味着 SUSE CaaS Platform 的设计不是为了弥补 Amazon EKS 或 Google Kubernetes Engine 的不足，而是为了让用户可以跨多个云和数据中心运行容器。&lt;/p&gt;

&lt;h4 id=&quot;telekube&quot;&gt;Telekube&lt;/h4&gt;

&lt;p&gt;Teleport SSH 服务器开发商 Gravitational 推出了 Telekube，这是一款在本地或远程集群上运行的“生产强化型”Kubernetes 发行版。Telekube 定位为私有 SaaS 平台解决方案，将 Kubernetes 作为跨多个区域运行的托管服务。&lt;/p&gt;

&lt;p&gt;Telekube 上的应用程序必须能够在 Kubernetes 容器中运行。他们还必须打包成“Bundle”，然后发布到 Kubernetes 集群中。在部署基于容器的应用程序之前，需要为捆绑做一些额外的工作，不过 Telekube 唯一留给用户的任务是维护 Bundle Manifest。&lt;/p&gt;

&lt;p&gt;转载请注明出处，本文采用 &lt;a href=&quot;http://creativecommons.org/licenses/by-nc-nd/4.0/&quot;&gt;CC4.0&lt;/a&gt; 协议授权&lt;/p&gt;</content><author><name>Jevic</name></author><summary type="html">12 款最突出的 Kubernetes 产品，也就是整合了 Kubernetes 和容器工具的发行版。</summary></entry></feed>